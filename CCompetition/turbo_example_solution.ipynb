{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from torch.quasirandom import SobolEngine\n",
    "import botorch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use the TuRBO Bayesian Optimization tool for the capstone project. TuRBO is a BO algorithm proposed by Uber that specializes in high-dimensional problems. You can read the details of the algorithm in the paper:\n",
    "\n",
    "\n",
    "Eriksson et al., \"Scalable Global Optimization via Local Bayesian Optimization\", NeurIPS (2019). URL: https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf\n",
    "\n",
    "For implementing the method, we will be using the Gaussian Process library GPyTorch, and the Bayesian Optimization library BoTorch. We will be loosely following the tutorial made by BoTorch's team:\n",
    "\n",
    "https://botorch.org/tutorials/turbo_1\n",
    "\n",
    "However, we will be making some modification that are case-specific for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TuRBO works by creating a Trust Region over which will focus all our optimization efforts. This works great for higher-dimensions because the search space is too large and algorithms tend to over-explore! \n",
    "\n",
    "We keep track of a 'Turbo State' that dictates the size and location of the region. The code below implements a data class that will help us keep track of the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a dataclass for our state\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int # dimension of the problem, aka input dimension\n",
    "    batch_size: int = 1 # we could do batch optimization, but the capstone only does one query at a time\n",
    "    length: float = 0.8 # the length of the current trust region\n",
    "    length_min: float = 0.5 ** 7 # minimum length for the trust region\n",
    "    length_max: float = 1.6 # maximum length for the trust region\n",
    "    failure_counter: int = 0 # initialize counter of the number of failures to improve on the best observation\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0 # initialize counter of the number of success to improve on the best observation\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3, this is the number of successes in a row needed to expand the region\n",
    "    best_value: float = -float(\"inf\") # best value so far, initialized to be the infimum\n",
    "    restart_triggered: bool = False \n",
    "\n",
    "    def __post_init__(self): # sets the failure tolerance based on the problem's dimension and batch size.\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size]) # number of failures needed in a row to shrink the trust region\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):  # updates the TurboState based on new observations (Y_next). \n",
    "    #It checks if the new observation is a success or a failure and accordingly updates the success and failure counters. \n",
    "    #It also adjusts the trust region's length based on these counters.\n",
    "    # count if a success, otherwise a failure\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "    # check if we need to expand or shrink the trust region\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "    # set the best value if we got a new observation\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be very important to keep track of the state when we optimize, as we will need to make sure we keep the state updated from query to query. You can use a print statement to see the value of a state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=6, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=6, success_counter=0, success_tolerance=10, best_value=-inf, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "state = TurboState(dim = 6)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to record these variables after choosing a new query, and re-input, and update to the correct state when we receive new observations. An example of this will be given later. We can then define the TuRBO loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    #selecting the next point for evaluation in Bayesian optimization allows for balancing exploration (trying out new areas of the search space) and exploitation (focusing on areas known to be promising). \n",
    "    #The use of a GP model enables the method to incorporate prior knowledge and the results of past evaluations to make informed decisions about where to search next.\n",
    "    #generates the next query point in the optimization process using Thompson Sampling (TS). \n",
    "    #It requires a GP model and training data as inputs and operates within the current trust region.\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size = 1, # fix batch size to 1\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the trust region to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "    # we focus only on thompson sampling as an acquisition function\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = (\n",
    "            torch.rand(n_candidates, dim)\n",
    "            <= prob_perturb\n",
    "        )\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),))] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask        \n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        posterior_distribution = model(X_cand)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            posterior_sample = posterior_distribution.sample()\n",
    "            X_next_idx = torch.argmax(posterior_sample)\n",
    "            X_next = X_cand[X_next_idx]\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above requires us to use a GPyTorch model as an input. A tutorial on how GPyTorch models can be used is found here: https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "Below we define our model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the model given in the tutorial, we also add the hyper-parameter training as a method\n",
    "class ExactGPModel(gpytorch.models.ExactGP): #  defines a Gaussian Process model with a constant mean and an RBF kernel. \n",
    "    #It's used in the optimization process to model the objective function.\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # set a constant mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # use a simple RBF kernel with constant scaling\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "        # set number of hyper-parameter training iterations\n",
    "        self.training_iter = 200\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, we will optimize the function:\n",
    "\n",
    "$$f(x_1, x_2, x_3, x_4, x_5, x_6) = x_3 * \\sin(x_1) * \\cos(x_2) + x_4 * x_5 - x_6 * x_5^2$$\n",
    "\n",
    "We will create an initial data set at random.\n",
    "\n",
    "Do not forget to re-define our state as we have a new best-observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example function is provided to illustrate the optimization process. It's a multi-dimensional function involving sine, cosine, and polynomial terms.\n",
    "func = lambda x: torch.sin(x[:, 0]) * torch.cos(x[:, 1]) * x[:, 2] + x[:, 3] * x[:, 4] - x[:, 5] * (x[:, 4]**2)\n",
    "\n",
    "train_x = torch.rand(size = torch.Size([15, 6]))\n",
    "train_y = func(train_x)\n",
    "\n",
    "state = TurboState(dim = 6, best_value = torch.max(train_y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train the hyper-parameters of the model. This can be done in a similar fashion to a normal PyTorch model.\n",
    "\n",
    "All we need is to define a model and a likelihood, and then activate .train() mode. We then follow classical PyTorch syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/200 - Loss: 0.616   lengthscale: tensor([[1.2320, 1.2288, 1.2312, 1.2162, 1.2171, 1.2289]])   noise: 0.339\n",
      "Iter 20/200 - Loss: 0.211   lengthscale: tensor([[1.9042, 1.9127, 1.9246, 1.6649, 1.6757, 1.8806]])   noise: 0.135\n",
      "Iter 30/200 - Loss: -0.081   lengthscale: tensor([[2.4718, 2.5382, 2.5508, 1.3199, 1.3589, 2.3572]])   noise: 0.052\n",
      "Iter 40/200 - Loss: -0.305   lengthscale: tensor([[2.8971, 3.0906, 3.0451, 0.5891, 0.6284, 2.6822]])   noise: 0.023\n",
      "Iter 50/200 - Loss: -0.361   lengthscale: tensor([[3.1547, 3.4642, 3.2982, 0.3669, 0.4352, 2.9549]])   noise: 0.012\n",
      "Iter 60/200 - Loss: -0.366   lengthscale: tensor([[3.1634, 3.5397, 3.1977, 0.4212, 0.4745, 2.9167]])   noise: 0.009\n",
      "Iter 70/200 - Loss: -0.381   lengthscale: tensor([[2.8167, 3.3782, 2.7320, 0.4494, 0.5047, 2.5991]])   noise: 0.008\n",
      "Iter 80/200 - Loss: -0.397   lengthscale: tensor([[2.0876, 3.1984, 2.1264, 0.5052, 0.5713, 2.4714]])   noise: 0.007\n",
      "Iter 90/200 - Loss: -0.425   lengthscale: tensor([[1.2178, 3.2938, 1.6378, 0.5489, 0.5617, 2.8791]])   noise: 0.006\n",
      "Iter 100/200 - Loss: -0.449   lengthscale: tensor([[0.9480, 3.7057, 1.2693, 0.5007, 0.5513, 3.4991]])   noise: 0.004\n",
      "Iter 110/200 - Loss: -0.467   lengthscale: tensor([[0.9344, 4.1717, 1.2711, 0.5179, 0.5279, 3.9898]])   noise: 0.003\n",
      "Iter 120/200 - Loss: -0.480   lengthscale: tensor([[0.9797, 4.6085, 1.3706, 0.5368, 0.5898, 4.3695]])   noise: 0.002\n",
      "Iter 130/200 - Loss: -0.488   lengthscale: tensor([[1.0432, 5.0263, 1.4456, 0.5626, 0.6019, 4.7240]])   noise: 0.002\n",
      "Iter 140/200 - Loss: -0.492   lengthscale: tensor([[1.0412, 5.4034, 1.4486, 0.5683, 0.6161, 5.0708]])   noise: 0.002\n",
      "Iter 150/200 - Loss: -0.494   lengthscale: tensor([[1.0468, 5.7246, 1.4441, 0.5704, 0.6165, 5.3913]])   noise: 0.001\n",
      "Iter 160/200 - Loss: -0.495   lengthscale: tensor([[1.0545, 5.9911, 1.4455, 0.5730, 0.6221, 5.6756]])   noise: 0.001\n",
      "Iter 170/200 - Loss: -0.495   lengthscale: tensor([[1.0647, 6.2170, 1.4569, 0.5771, 0.6269, 5.9293]])   noise: 0.001\n",
      "Iter 180/200 - Loss: -0.496   lengthscale: tensor([[1.0728, 6.4168, 1.4606, 0.5799, 0.6307, 6.1590]])   noise: 0.001\n",
      "Iter 190/200 - Loss: -0.496   lengthscale: tensor([[1.0753, 6.6000, 1.4641, 0.5809, 0.6332, 6.3687]])   noise: 0.001\n",
      "Iter 200/200 - Loss: -0.496   lengthscale: tensor([[1.0772, 6.7731, 1.4635, 0.5815, 0.6349, 6.5599]])   noise: 0.001\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "#training the hyperparameters of the GP model with a likelihood function and an optimizer. \n",
    "#It also calculates the marginal log likelihood as a loss function.\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "for i in range(model.training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 10 == 9:\n",
    "        print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale.detach()}   noise: %.3f' % (\n",
    "            i + 1, model.training_iter, loss.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that takes as input:\n",
    "1. Training Data\n",
    "2. A TuRBO State\n",
    "\n",
    "And returns the next suggested query! We will define the GP model and optimize the GP's hyper-parameters inside the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_query_via_TurBO(train_x, train_y, turbo_state, verbose = False):  # combines all the previous elements to suggest the next query point in the optimization process. \n",
    "    #It trains a GP model on the current training data and uses the generate_batch function to propose the next point.\n",
    "    # the verbose variable decides wether to print the hyper-parameter optimization details\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # demonstrates how to iteratively query new points and update the training data and the TurboState. \n",
    "    # After each query, the state is updated with the new observation.\n",
    "    for i in range(model.training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if (i % 10 == 9) & verbose:\n",
    "            print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale}   noise: %.3f' % (\n",
    "                i + 1, model.training_iter, loss.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "    \n",
    "    return generate_batch(turbo_state, model = model, X = train_x, Y = train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can obtain a suggested query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next chose query: tensor([0.8103, 0.4809, 0.4493, 0.9277, 0.9918, 0.8265])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdche\\AppData\\Roaming\\Python\\Python310\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\jdche\\AppData\\Roaming\\Python\\Python310\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)\n",
    "print(f'Next chose query: {next_query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep track of the state, and also update it when we receive new information. For example, the state that was used to choose the query can be printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State at latest optimization loop:\n",
      "\n",
      "TurboState(dim=6, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=6, success_counter=0, success_tolerance=10, best_value=tensor(0.7864), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "print('State at latest optimization loop:')\n",
    "print()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what observation we would have gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3956])\n"
     ]
    }
   ],
   "source": [
    "y_next = func(next_query.reshape(1, -1))\n",
    "print(y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new observation, it is vital to update the state before optimizing again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = update_state(state, y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now optimize again to obtain the next point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New query: tensor([0.9784, 0.4101, 0.9316, 0.8540, 0.9870, 0.2342])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.concat((train_x, next_query.reshape(1, -1)), dim = 0)\n",
    "train_y = torch.concat((train_y, y_next))\n",
    "\n",
    "next_next_query = next_query_via_TurBO(train_x, train_y, new_state)\n",
    "print(f'New query:', next_next_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel TuRBO would help you with the high-dimensional problems in the Capstone, give it a go! There a few questions that may lead to better performance:\n",
    "\n",
    "1. Maybe you can constraint some of the GPs hyper-parameters for better behaviour?\n",
    "2. How are you planning to initialize the Turbo State for the first time?\n",
    "\n",
    "So take the code from this notebook and modify it to your liking!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df83ed6fabd41a8e562c5a64e44b5d97b19c29150cbf5eb47fd88445500a37c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
