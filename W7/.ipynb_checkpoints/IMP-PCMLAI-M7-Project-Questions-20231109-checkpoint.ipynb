{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631ac862",
   "metadata": {
    "id": "631ac862"
   },
   "outputs": [],
   "source": [
    "#Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943eec6",
   "metadata": {
    "id": "2943eec6"
   },
   "source": [
    "## We will work with the \"Adult\" Data Set of the UCI Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75906b0e",
   "metadata": {
    "id": "75906b0e"
   },
   "source": [
    "Here we will try to predict whether one's salary is above or below $50,000/year. \n",
    "The original dataset can be found on UCI Repository: https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a58790",
   "metadata": {
    "id": "25a58790"
   },
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7755f3f1",
   "metadata": {
    "id": "7755f3f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age         Workclass  fnlwgt  Education  Education-Num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       Martial Status         Occupation   Relationship   Race     Sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   Capital Gain  Capital Loss  Hours per week        Country Target  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n"
     ]
    }
   ],
   "source": [
    "# Source: https://www.valentinmihov.com/2015/04/17/adult-income-data-set/\n",
    "# taken and modified slightly from https://fairmlbook.org/code/adult.html\n",
    "\n",
    "features = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"] #feaures of the dataset \n",
    "\n",
    "# Change these to local file if available\n",
    "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data' #link of the training data\n",
    "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test' #link of the test data\n",
    "\n",
    "# This will download 3.8M\n",
    "df_train = pd.read_csv(train_url, names=features, sep=r'\\s*,\\s*', \n",
    "                             engine='python', na_values=\"?\") #read csv training\n",
    "# This will download 1.9M\n",
    "df_test = pd.read_csv(test_url, names=features, sep=r'\\s*,\\s*', \n",
    "                            engine='python', na_values=\"?\", skiprows=1) #read csv test\n",
    "\n",
    "num_train = len(df_train) #number of training instances\n",
    "#print(f'num_train={num_train}')\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e81f9a",
   "metadata": {
    "id": "b2e81f9a"
   },
   "source": [
    "### Q1: Replace the labels of the training and test sets so that \"Target = 1\" means high income and \"Target = 0\" means low income. The target column should be an integer (binary) column eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74068666",
   "metadata": {
    "id": "74068666"
   },
   "source": [
    "#### A1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0d9a5e",
   "metadata": {
    "id": "5d0d9a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Workclass  fnlwgt     Education  Education-Num      Martial Status  \\\n",
      "0   25    Private  226802          11th              7       Never-married   \n",
      "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
      "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
      "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
      "4   18        NaN  103497  Some-college             10       Never-married   \n",
      "\n",
      "          Occupation Relationship   Race     Sex  Capital Gain  Capital Loss  \\\n",
      "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
      "1    Farming-fishing      Husband  White    Male             0             0   \n",
      "2    Protective-serv      Husband  White    Male             0             0   \n",
      "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
      "4                NaN    Own-child  White  Female             0             0   \n",
      "\n",
      "   Hours per week        Country  Target  \n",
      "0              40  United-States       0  \n",
      "1              50  United-States       0  \n",
      "2              40  United-States       1  \n",
      "3              40  United-States       1  \n",
      "4              30  United-States       0  \n"
     ]
    }
   ],
   "source": [
    "#now we can replace the labels\n",
    "df_train.Target = df_train.Target.replace({'>50K':1,'<=50K':0})\n",
    "df_test.Target = df_test.Target.replace({'>50K.':1,'<=50K.':0})\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256144fd",
   "metadata": {
    "id": "256144fd"
   },
   "source": [
    "### Q2: Inspect the column names and the data type of each column. Answer the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2af7a",
   "metadata": {
    "id": "23c2af7a"
   },
   "source": [
    "#### Q2.1: List how many missing values there are in each column for training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fc3bf",
   "metadata": {
    "id": "9c5fc3bf"
   },
   "source": [
    "#### A2.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca8a1f4",
   "metadata": {
    "id": "dca8a1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Age             32561 non-null  int64 \n",
      " 1   Workclass       30725 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   Education       32561 non-null  object\n",
      " 4   Education-Num   32561 non-null  int64 \n",
      " 5   Martial Status  32561 non-null  object\n",
      " 6   Occupation      30718 non-null  object\n",
      " 7   Relationship    32561 non-null  object\n",
      " 8   Race            32561 non-null  object\n",
      " 9   Sex             32561 non-null  object\n",
      " 10  Capital Gain    32561 non-null  int64 \n",
      " 11  Capital Loss    32561 non-null  int64 \n",
      " 12  Hours per week  32561 non-null  int64 \n",
      " 13  Country         31978 non-null  object\n",
      " 14  Target          32561 non-null  int64 \n",
      "dtypes: int64(7), object(8)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#first list the columns\n",
    "pd.set_option('max_colwidth', None)\n",
    "df_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab5b9f2",
   "metadata": {
    "id": "1ab5b9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Training Set:\n",
      "Age                  0\n",
      "Workclass         1836\n",
      "fnlwgt               0\n",
      "Education            0\n",
      "Education-Num        0\n",
      "Martial Status       0\n",
      "Occupation        1843\n",
      "Relationship         0\n",
      "Race                 0\n",
      "Sex                  0\n",
      "Capital Gain         0\n",
      "Capital Loss         0\n",
      "Hours per week       0\n",
      "Country            583\n",
      "Target               0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now see the number of missing values for each column -- training\n",
    "print(\"Missing Values in Training Set:\")\n",
    "missing_values_train = df_train.isnull().sum()\n",
    "print(missing_values_train)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65a73ed",
   "metadata": {
    "id": "a65a73ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Training Set:\n",
      "Age                 0\n",
      "Workclass         963\n",
      "fnlwgt              0\n",
      "Education           0\n",
      "Education-Num       0\n",
      "Martial Status      0\n",
      "Occupation        966\n",
      "Relationship        0\n",
      "Race                0\n",
      "Sex                 0\n",
      "Capital Gain        0\n",
      "Capital Loss        0\n",
      "Hours per week      0\n",
      "Country           274\n",
      "Target              0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#see the number of missing values for each column -- test\n",
    "print(\"Missing Values in Training Set:\")\n",
    "missing_values_test = df_test.isnull().sum()\n",
    "print(missing_values_test)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567035d",
   "metadata": {
    "id": "4567035d"
   },
   "source": [
    "#### Q2.2: How many missing values there are in total for the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ae857",
   "metadata": {
    "id": "797ae857"
   },
   "source": [
    "#### A2.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e542ea52",
   "metadata": {
    "id": "e542ea52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Values in Training Set: 4262\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of missing values in the training set\n",
    "total_missing_values_train = df_train.isnull().sum().sum()\n",
    "print(\"Total Missing Values in Training Set:\", total_missing_values_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ec48d",
   "metadata": {
    "id": "797ec48d"
   },
   "source": [
    "#### Q2.3: How many rows are there with at least one NaN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b35ef",
   "metadata": {
    "id": "146b35ef"
   },
   "source": [
    "#### A2.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8ddf2e",
   "metadata": {
    "id": "3e8ddf2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows with at least one NaN in Training Set: 2399\n"
     ]
    }
   ],
   "source": [
    "#one possible way:\n",
    "#take a True/False Series for whether each row includes NaN. Then sum them, where True will be taken as = 1.\n",
    "rows_with_nan_train = df_train.isnull().any(axis=1).sum()\n",
    "print(\"Number of Rows with at least one NaN in Training Set:\", rows_with_nan_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b211c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_train=32561,Number of rows in df_test = 16281 \n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows in df_train={len(df_train)},Number of rows in df_test = {len(df_test)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9f381",
   "metadata": {
    "id": "23e9f381"
   },
   "source": [
    "#### Q2.4: If NaN's were identially distributed among the NaN rows at random, then what is the expectation of the number of NaNs in a row that has at least one NaN? If you find the same expectation for the test set, do these values look alike?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcc5a4",
   "metadata": {
    "id": "5adcc5a4"
   },
   "source": [
    "#### A2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea07117",
   "metadata": {
    "id": "fea07117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation of NaNs per row in Training Set: 1.7765735723218008\n",
      "Expectation of NaNs per row in Test Set: 1.8042588042588044\n"
     ]
    }
   ],
   "source": [
    "# For the training set\n",
    "total_nan_train = df_train.isnull().sum().sum()\n",
    "rows_with_nan_train = df_train.isnull().any(axis=1).sum()\n",
    "expectation_train = total_nan_train / rows_with_nan_train\n",
    "\n",
    "# For the test set\n",
    "total_nan_test = df_test.isnull().sum().sum()\n",
    "rows_with_nan_test = df_test.isnull().any(axis=1).sum()\n",
    "expectation_test = total_nan_test / rows_with_nan_test\n",
    "\n",
    "print(\"Expectation of NaNs per row in Training Set:\", expectation_train)\n",
    "print(\"Expectation of NaNs per row in Test Set:\", expectation_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49afb50",
   "metadata": {
    "id": "f49afb50"
   },
   "source": [
    "### Q3: Let us inspect the target variable further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61367d5f",
   "metadata": {
    "id": "61367d5f"
   },
   "source": [
    "#### Q3.1: What fraction of the training instances are making high income? Does a similar result hold for the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68233a89",
   "metadata": {
    "id": "68233a89"
   },
   "source": [
    "#### A3.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc1ae0d",
   "metadata": {
    "id": "6cc1ae0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2408095574460244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Target\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e9804f",
   "metadata": {
    "id": "80e9804f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23622627602727106"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df_test.head())\n",
    "df_test[\"Target\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3366487",
   "metadata": {
    "id": "d3366487"
   },
   "source": [
    "#### Q3.2: If we classify everyone as a low-income instance, what is our accuracy, sensitivity, and specificy in the test set? Assume that a \"positive\" class is a high income class, that is, our end goal is to be able to tell who makes more money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79e4f3",
   "metadata": {
    "id": "3d79e4f3"
   },
   "source": [
    "#### A3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e69bf06",
   "metadata": {
    "id": "1e69bf06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_test=0.7637737239727289,sensitivity=0, specificity=1 \n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of instances in the test set\n",
    "total_test_instances = len(df_test)\n",
    "\n",
    "# Calculate the number of true negatives (low-income instances)\n",
    "true_negatives_test = (df_test['Target'] == 0).sum()\n",
    "\n",
    "# Calculate the accuracy\n",
    "\n",
    "accuracy_test = true_negatives_test / total_test_instances\n",
    "sensitivity = 0 #True Positives (TP): 0, because the classifier never predicts high income.\n",
    "specificity = 1 #Ture Negative (TN): 0, because we classify everyone as low income, all low-income instances are collrectly identified. So the specificity in this case would be 1.\n",
    "\n",
    "print(f'accuracy_test={accuracy_test},sensitivity={sensitivity}, specificity={specificity} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bc8cc",
   "metadata": {
    "id": "a87bc8cc"
   },
   "source": [
    "#### Q3.3: List the fraction of high income instances within each group of \"workclass\" separately in the training set. Do you have an intuition on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650ccc",
   "metadata": {
    "id": "3e650ccc"
   },
   "source": [
    "#### A3.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6693e54c",
   "metadata": {
    "id": "6693e54c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>0.386458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local-gov</td>\n",
       "      <td>0.294792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never-worked</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.218673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>0.557348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0.284927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>State-gov</td>\n",
       "      <td>0.271957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Without-pay</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Workclass    Target\n",
       "0       Federal-gov  0.386458\n",
       "1         Local-gov  0.294792\n",
       "2      Never-worked  0.000000\n",
       "3           Private  0.218673\n",
       "4      Self-emp-inc  0.557348\n",
       "5  Self-emp-not-inc  0.284927\n",
       "6         State-gov  0.271957\n",
       "7       Without-pay  0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('Workclass', as_index=False)['Target'].mean() #or you can find this via alternative codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ad4b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>0.402542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local-gov</td>\n",
       "      <td>0.297220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never-worked</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.216236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>0.545769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0.267222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>State-gov</td>\n",
       "      <td>0.259151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Without-pay</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Workclass    Target\n",
       "0       Federal-gov  0.402542\n",
       "1         Local-gov  0.297220\n",
       "2      Never-worked  0.000000\n",
       "3           Private  0.216236\n",
       "4      Self-emp-inc  0.545769\n",
       "5  Self-emp-not-inc  0.267222\n",
       "6         State-gov  0.259151\n",
       "7       Without-pay  0.285714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('Workclass', as_index=False)['Target'].mean() #or you can find this via alternative codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e568da8",
   "metadata": {
    "id": "7e568da8"
   },
   "source": [
    "#### Q3.4: Motivated by the question before, apply the following simple classification on the test set: if an instance has a workclass that is in the first or second highest-making group, then classify \"1\". What is the accuracy and the sensitivity of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea66e1",
   "metadata": {
    "id": "f9ea66e1"
   },
   "source": [
    "#### A3.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52412c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Two High-Income Workclasses:\n",
      "['Self-emp-inc', 'Federal-gov']\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Workclass' and calculate the mean of 'Target', with as_index=False\n",
    "workclass_income = df_train.groupby('Workclass', as_index=False)['Target'].mean()\n",
    "\n",
    "# Sort the results in descending order based on 'Target' mean\n",
    "sorted_workclass_income = workclass_income.sort_values(by='Target', ascending=False)\n",
    "\n",
    "# Select the top two entries\n",
    "top_two_workclasses = sorted_workclass_income.head(2)['Workclass']\n",
    "# If you need it as a list\n",
    "#print(top_two_workclasses)\n",
    "top_two_workclasses_list = top_two_workclasses.tolist()\n",
    "\n",
    "print(\"Top Two High-Income Workclasses:\")\n",
    "print(top_two_workclasses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5e48d61",
   "metadata": {
    "id": "a5e48d61"
   },
   "outputs": [],
   "source": [
    "greedy_workclass = df_test.apply(lambda row: 1 if row['Workclass'] in top_two_workclasses_list else 0, axis = 1) #apply a function to predict -- fill the dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae291f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int64\n",
      "1051\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "16276    0\n",
      "16277    0\n",
      "16278    0\n",
      "16279    0\n",
      "16280    1\n",
      "Length: 16281, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(greedy_workclass.head())\n",
    "print(greedy_workclass.sum())\n",
    "print(greedy_workclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421d3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_workclass_alt = []\n",
    "for _, row in df_test.iterrows():\n",
    "    if row['Workclass'] in top_two_workclasses_list:\n",
    "        #print(row['Workclass'])\n",
    "        greedy_workclass_alt.append(1)\n",
    "    else:\n",
    "        greedy_workclass_alt.append(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e5a384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int64\n",
      "greedy_workclass_alt.sum()= 1051\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "16276    0\n",
      "16277    0\n",
      "16278    0\n",
      "16279    0\n",
      "16280    1\n",
      "Length: 16281, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "greedy_workclass_alt = pd.Series(greedy_workclass_alt)\n",
    "print(greedy_workclass_alt.head())\n",
    "print(f'greedy_workclass_alt.sum()= {greedy_workclass_alt.sum()}')\n",
    "print(greedy_workclass_alt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0cc9639",
   "metadata": {
    "id": "b0cc9639"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix #import sklearn's confusion matrix\n",
    "\n",
    "# Assuming df_test['Target'] contains the true labels\n",
    "# And greedy_workclass contains your predicted labels\n",
    "cm = confusion_matrix(df_test['Target'], greedy_workclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c65cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11890   545]\n",
      " [ 3340   506]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49417cc3",
   "metadata": {
    "id": "49417cc3"
   },
   "outputs": [],
   "source": [
    "total = len(df_test) #number of instances\n",
    "correct = cm[0,0]+cm[1,1] #number of collectly classified instances -- sum of the ??\n",
    "accuracy = correct / total\n",
    "\n",
    "TP = cm[0, 0]  # True Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "sensitivity = TP / (TP + FN)  #fraction of 1's that we can find out of true 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21d92e8d",
   "metadata": {
    "id": "21d92e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7613782937166022 , and Sensitivity: 0.7806959947472094\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy, \", and Sensitivity:\", sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7656c1e",
   "metadata": {
    "id": "e7656c1e"
   },
   "source": [
    "### Q4: Further statistics and Outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb01a85",
   "metadata": {
    "id": "cfb01a85"
   },
   "source": [
    "#### Q4.1: List the fraction of male and females within US citizens in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8b941",
   "metadata": {
    "id": "92e8b941"
   },
   "source": [
    "#### A4.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee7c43c",
   "metadata": {
    "id": "bee7c43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of Males among US citizens: 0.6680836475831333\n",
      "Fraction of Females among US citizens: 0.3319163524168666\n"
     ]
    }
   ],
   "source": [
    "# Filter for US citizens\n",
    "us_citizens = df_train[df_train['Country'] == 'United-States']\n",
    "\n",
    "# Group by 'Sex' and count the number of instances\n",
    "sex_counts = us_citizens['Sex'].value_counts()\n",
    "\n",
    "# Calculate fractions\n",
    "fraction_male = sex_counts['Male'] / us_citizens.shape[0]\n",
    "fraction_female = sex_counts['Female'] / us_citizens.shape[0]\n",
    "\n",
    "print(\"Fraction of Males among US citizens:\", fraction_male)\n",
    "print(\"Fraction of Females among US citizens:\", fraction_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1c7f0",
   "metadata": {
    "id": "89f1c7f0"
   },
   "source": [
    "#### Q4.2: What is the most common occupation (training set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018caa6",
   "metadata": {
    "id": "2018caa6"
   },
   "source": [
    "#### A4.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4e270a5",
   "metadata": {
    "id": "f4e270a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Occupation: Prof-specialty\n"
     ]
    }
   ],
   "source": [
    "# Find the most common occupation\n",
    "most_common_occupation = df_train['Occupation'].value_counts().idxmax()\n",
    "\n",
    "print(\"Most Common Occupation:\", most_common_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cb992",
   "metadata": {
    "id": "0c0cb992"
   },
   "source": [
    "#### Q4.3: Which occupations are the most common male and female instances, respectively (training set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcae06",
   "metadata": {
    "id": "58fcae06"
   },
   "source": [
    "#### A4.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf59b47",
   "metadata": {
    "id": "bdf59b47"
   },
   "outputs": [],
   "source": [
    "# Find the most common occupation for females\n",
    "most_common_occupation_female = df_train[df_train[\"Sex\"]==\"Female\"][\"Occupation\"].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800dd8cb",
   "metadata": {
    "id": "800dd8cb"
   },
   "outputs": [],
   "source": [
    "# Find the most common occupation for males\n",
    "most_common_occupation_male = df_train[df_train[\"Sex\"]==\"Male\"][\"Occupation\"].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "871bd609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Occupation for Males: Craft-repair\n",
      "Most Common Occupation for Females: Adm-clerical\n"
     ]
    }
   ],
   "source": [
    "print(\"Most Common Occupation for Males:\", most_common_occupation_male)\n",
    "print(\"Most Common Occupation for Females:\", most_common_occupation_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab38b5",
   "metadata": {
    "id": "13ab38b5"
   },
   "source": [
    "#### Q4.4: Plot the histogram of the \"Age\" column (using training set data). Let the histogram have 10 bins, and reflect the percentage of instances falling in the relevant bin on Y axis.  Visually try to see if there are outliers. Are there more outliers on the high age, or low age?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ca4f6",
   "metadata": {
    "id": "3a5ca4f6"
   },
   "source": [
    "#### A4.4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5e9f6b0",
   "metadata": {
    "id": "a5e9f6b0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqYUlEQVR4nO3de3TU5YH/8c+EhIFoAgsYkmAIl6WrXIoIcqtyOd2EDSwrqyglWy7Lui5baoEsiwbhMCmtobZbI8WFsy2CmE1he7hIBUvC1iRlBSpgKHAwhmMkFJJyUMkA+TEMyfP7w5OpQy4wOEOezLxf5+Tg9/t9vk+eTy6Tj9+5OYwxRgAAABaLausFAAAA3AqFBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgvei2XkCwNDQ06Pz584qLi5PD4Wjr5QAAgNtgjNHly5eVnJysqKiWr6OETWE5f/68UlJS2noZAADgDpw9e1b3339/i8fDprDExcVJ+iJwfHx8G6+meV6vV4WFhUpPT1dMTExbL+euicTckZhZIje5w18kZpZCm9vtdislJcX3d7wlYVNYGu8Gio+Pt7qwxMbGKj4+PuJ+0CMtdyRmlshN7vAXiZmlu5P7Vg/n4EG3AADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANaLbusFAO1dnxd2N9nn7GD08khpsGuvPPWtv2V6W/hk9ZS2XgIABIQrLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1uNpzbBGc08PBgBA4goLAABoBygsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADr8cJxYcqmF2FzdjB6eaQ02LVXnnpHWy8HANAOcYUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6AReW0tJSTZ06VcnJyXI4HNq5c6ffcYfD0ezHj3/84xbn3LRpU7PnXLt2LeBAAAAg/ARcWK5evaqhQ4dq7dq1zR6vrq72+3j99dflcDj05JNPtjpvfHx8k3M7deoU6PIAAEAYCviF4zIyMpSRkdHi8cTERL/tt956SxMnTlS/fv1andfhcDQ5FwAAQArxK93+6U9/0u7du/XGG2/ccuyVK1eUmpqq+vp6PfTQQ1q1apWGDRvW4niPxyOPx+PbdrvdkiSv1yuv1/vVFx8Cjeu6G+tzdjAh/xy3yxll/P6NBLZnDtXP4N38GbcJuSMndyRmlkKb+3bndBhj7vgW1eFwaMeOHZo2bVqzx19++WWtXr1a58+fb/XunYMHD+r06dMaMmSI3G63Xn31Ve3Zs0fHjh3TgAEDmj3H5XIpJyenyf6CggLFxsbeUR4AAHB31dXVKTMzU7W1tYqPj29xXEgLywMPPKC0tDT97Gc/C2jehoYGPfzwwxo3bpzWrFnT7JjmrrCkpKTo4sWLrQZuS16vV0VFRUpLS1NMTExIP9dg196Qzh8IZ5TRqhENWnE4Sp6GyHgvIdszn3BNCsm8d/Nn3CbkjpzckZhZCm1ut9utHj163LKwhOwuod/97ncqLy/X1q1bAz43KipKjzzyiCoqKloc43Q65XQ6m+yPiYmx/ofobqzRxjcZ9DQ4rFxXKNmaOdQ/f+3h9zAUyB05IjGzFJrctztfyF6HZcOGDRo+fLiGDh0a8LnGGJWVlSkpKSkEKwMAAO1NwFdYrly5otOnT/u2KysrVVZWpm7duql3796Svri886tf/Ur/8R//0ewcs2fPVq9evZSbmytJysnJ0ejRozVgwAC53W6tWbNGZWVleu211+4kEwAACDMBF5bDhw9r4sSJvu2srCxJ0pw5c7Rp0yZJ0pYtW2SM0cyZM5udo6qqSlFRf764c+nSJT377LOqqalRly5dNGzYMJWWlmrkyJGBLg8AAIShgAvLhAkTdKvH6T777LN69tlnWzxeXFzst/3KK6/olVdeCXQpAAAgQoT0dVjCRZ8XdgdlHmcHo5dHfvEMHhsfiAkAgK1480MAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsF7AhaW0tFRTp05VcnKyHA6Hdu7c6Xd87ty5cjgcfh+jR4++5bzbtm3TwIED5XQ6NXDgQO3YsSPQpQEAgDAVcGG5evWqhg4dqrVr17Y45m/+5m9UXV3t+9izZ0+rcx44cEAzZszQrFmzdOzYMc2aNUtPP/20Dh06FOjyAABAGIoO9ISMjAxlZGS0OsbpdCoxMfG258zLy1NaWpqys7MlSdnZ2SopKVFeXp5++ctfBrpEAAAQZgIuLLejuLhYCQkJ6tq1q8aPH68f/vCHSkhIaHH8gQMHtHjxYr99kyZNUl5eXovneDweeTwe37bb7ZYkeb1eeb3erxbgJs4OJjjzRBm/fyNFJOa2PXOwf0dunjdU89uK3JGTOxIzS6HNfbtzOowxd3yL6nA4tGPHDk2bNs23b+vWrbr33nuVmpqqyspKrVixQjdu3NCRI0fkdDqbnadjx47atGmTMjMzffsKCgr0j//4j36l5MtcLpdycnKa7C8oKFBsbOydRgIAAHdRXV2dMjMzVVtbq/j4+BbHBf0Ky4wZM3z/PXjwYI0YMUKpqanavXu3nnjiiRbPczgcftvGmCb7viw7O1tZWVm+bbfbrZSUFKWnp7ca+E4Mdu0NyjzOKKNVIxq04nCUPA0tZws3kZjb9swnXJNCMq/X61VRUZHS0tIUExMTks9hI3JHTu5IzCyFNnfjPSS3EpK7hL4sKSlJqampqqioaHFMYmKiampq/PZduHBBPXv2bPEcp9PZ7BWbmJiYoH8xPfXB/YPjaXAEfc72IBJz25o51De0ofg9bA/IHTkiMbMUmty3O1/IX4fl008/1dmzZ5WUlNTimDFjxqioqMhvX2FhocaOHRvq5QEAgHYg4CssV65c0enTp33blZWVKisrU7du3dStWze5XC49+eSTSkpK0ieffKJly5apR48e+vu//3vfObNnz1avXr2Um5srSVq4cKHGjRunH/3oR3r88cf11ltvad++fdq/f38QIgIAgPYu4MJy+PBhTZw40bfd+DiSOXPmaN26dTp+/Lg2b96sS5cuKSkpSRMnTtTWrVsVFxfnO6eqqkpRUX++uDN27Fht2bJFy5cv14oVK9S/f39t3bpVo0aN+irZAABAmAi4sEyYMEGtPbFo795bP0C1uLi4yb7p06dr+vTpgS4HAABEAN5LCAAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrBVxYSktLNXXqVCUnJ8vhcGjnzp2+Y16vV88//7yGDBmie+65R8nJyZo9e7bOnz/f6pybNm2Sw+Fo8nHt2rWAAwEAgPATcGG5evWqhg4dqrVr1zY5VldXp6NHj2rFihU6evSotm/fro8++kh/93d/d8t54+PjVV1d7ffRqVOnQJcHAADCUHSgJ2RkZCgjI6PZY126dFFRUZHfvp/97GcaOXKkqqqq1Lt37xbndTgcSkxMDHQ5AAAgAgRcWAJVW1srh8Ohrl27tjruypUrSk1NVX19vR566CGtWrVKw4YNa3G8x+ORx+Pxbbvdbklf3C3l9XqDsvZGzg4mOPNEGb9/I0Uk5rY9c7B/R26eN1Tz24rckZM7EjNLoc19u3M6jDF3fIvqcDi0Y8cOTZs2rdnj165d06OPPqoHHnhA+fn5Lc5z8OBBnT59WkOGDJHb7darr76qPXv26NixYxowYECz57hcLuXk5DTZX1BQoNjY2DvKAwAA7q66ujplZmaqtrZW8fHxLY4LWWHxer166qmnVFVVpeLi4lYXcbOGhgY9/PDDGjdunNasWdPsmOausKSkpOjixYsBfa7bMdi1NyjzOKOMVo1o0IrDUfI0OIIyZ3sQibltz3zCNSkk83q9XhUVFSktLU0xMTEh+Rw2Infk5I7EzFJoc7vdbvXo0eOWhSUkdwl5vV49/fTTqqys1G9/+9uAC0RUVJQeeeQRVVRUtDjG6XTK6XQ22R8TExP0L6anPrh/cDwNjqDP2R5EYm5bM4f6hjYUv4ftAbkjRyRmlkKT+3bnC/rrsDSWlYqKCu3bt0/du3cPeA5jjMrKypSUlBTs5QEAgHYo4CssV65c0enTp33blZWVKisrU7du3ZScnKzp06fr6NGjevvtt1VfX6+amhpJUrdu3dSxY0dJ0uzZs9WrVy/l5uZKknJycjR69GgNGDBAbrdba9asUVlZmV577bVgZAQAAO1cwIXl8OHDmjhxom87KytLkjRnzhy5XC7t2rVLkvTQQw/5nffuu+9qwoQJkqSqqipFRf354s6lS5f07LPPqqamRl26dNGwYcNUWlqqkSNHBro8AAAQhgIuLBMmTFBrj9O9ncfwFhcX+22/8soreuWVVwJdCoA71OeF3SGZ19nB6OWRXzxQPdiP3flk9ZSgzgegfeG9hAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgvYALS2lpqaZOnark5GQ5HA7t3LnT77gxRi6XS8nJyercubMmTJigkydP3nLebdu2aeDAgXI6nRo4cKB27NgR6NIAAECYCriwXL16VUOHDtXatWubPf7yyy/rpz/9qdauXav3339fiYmJSktL0+XLl1uc88CBA5oxY4ZmzZqlY8eOadasWXr66ad16NChQJcHAADCUHSgJ2RkZCgjI6PZY8YY5eXl6cUXX9QTTzwhSXrjjTfUs2dPFRQU6F/+5V+aPS8vL09paWnKzs6WJGVnZ6ukpER5eXn65S9/GegSAQBAmAm4sLSmsrJSNTU1Sk9P9+1zOp0aP3683nvvvRYLy4EDB7R48WK/fZMmTVJeXl6Ln8vj8cjj8fi23W63JMnr9crr9X6FFE05O5jgzBNl/P6NFJGYOxIzS6HNHezf62BqXJvNawyFSMwdiZml0Oa+3TmDWlhqamokST179vTb37NnT505c6bV85o7p3G+5uTm5ionJ6fJ/sLCQsXGxgay7Ft6eWRQp9OqEQ3BnbCdiMTckZhZCk3uPXv2BH3OYCsqKmrrJbSJSMwdiZml0OSuq6u7rXFBLSyNHA6H37Yxpsm+r3pOdna2srKyfNtut1spKSlKT09XfHz8Hay6ZYNde4MyjzPKaNWIBq04HCVPQ+tfj3ASibkjMbMU2twnXJOCOl8web1eFRUVKS0tTTExMW29nLsmEnNHYmYptLkb7yG5laAWlsTERElfXDFJSkry7b9w4UKTKyg3n3fz1ZRbneN0OuV0Opvsj4mJCfoX01Mf3BteT4Mj6HO2B5GYOxIzS6HJ3R7+OITi9qc9iMTckZhZCk3u250vqK/D0rdvXyUmJvpdMrp+/bpKSko0duzYFs8bM2ZMk8tMhYWFrZ4DAAAiR8BXWK5cuaLTp0/7tisrK1VWVqZu3bqpd+/eWrRokV566SUNGDBAAwYM0EsvvaTY2FhlZmb6zpk9e7Z69eql3NxcSdLChQs1btw4/ehHP9Ljjz+ut956S/v27dP+/fuDEBEAALR3AReWw4cPa+LEib7txseRzJkzR5s2bdLSpUv1//7f/9N3vvMdff755xo1apQKCwsVFxfnO6eqqkpRUX++uDN27Fht2bJFy5cv14oVK9S/f39t3bpVo0aN+irZAABAmAi4sEyYMEHGtPyURYfDIZfLJZfL1eKY4uLiJvumT5+u6dOnB7ocAAAQAXgvIQAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9aLbegEAcDv6vLC7rZfQImcHo5dHSoNde+Wpd/j2f7J6ShuuCggvXGEBAADWo7AAAADrUVgAAID1KCwAAMB6QS8sffr0kcPhaPKxYMGCZscXFxc3O/7DDz8M9tIAAEA7FfRnCb3//vuqr6/3bZ84cUJpaWl66qmnWj2vvLxc8fHxvu377rsv2EsDAADtVNALy81FY/Xq1erfv7/Gjx/f6nkJCQnq2rVrsJcDAADCQEhfh+X69evKz89XVlaWHA5Hq2OHDRuma9euaeDAgVq+fLkmTpzY6niPxyOPx+PbdrvdkiSv1yuv1/vVF/8lzg4mOPNEGb9/I0Uk5o7EzBK5b84d7Nsi2zTmC/ecXxaJmaXQ5r7dOR3GmJDdsvzP//yPMjMzVVVVpeTk5GbHlJeXq7S0VMOHD5fH49Gbb76p9evXq7i4WOPGjWtxbpfLpZycnCb7CwoKFBsbG7QMAAAgdOrq6pSZmana2lq/h4bcLKSFZdKkSerYsaN+/etfB3Te1KlT5XA4tGvXrhbHNHeFJSUlRRcvXmw18J0Y7NoblHmcUUarRjRoxeEoeRpav+IUTiIxdyRmlsh9c+4TrkltuKrQ83q9KioqUlpammJiYtp6OXdFJGaWQpvb7XarR48etywsIbtL6MyZM9q3b5+2b98e8LmjR49Wfn5+q2OcTqecTmeT/TExMUH/Yn75pbaDMl+DI+hztgeRmDsSM0vkbhQpf9BCcbtru0jMLIUm9+3OF7LXYdm4caMSEhI0ZUrg76XxwQcfKCkpKQSrAgAA7VFIrrA0NDRo48aNmjNnjqKj/T9Fdna2zp07p82bN0uS8vLy1KdPHw0aNMj3IN1t27Zp27ZtoVgaAABoh0JSWPbt26eqqirNmzevybHq6mpVVVX5tq9fv64lS5bo3Llz6ty5swYNGqTdu3dr8uTJoVgaAABoh0JSWNLT09XSY3k3bdrkt7106VItXbo0FMsAAABhgvcSAgAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1gl5YXC6XHA6H30diYmKr55SUlGj48OHq1KmT+vXrp/Xr1wd7WQAAoB2LDsWkgwYN0r59+3zbHTp0aHFsZWWlJk+erH/+539Wfn6+/u///k/f+c53dN999+nJJ58MxfIAAEA7E5LCEh0dfcurKo3Wr1+v3r17Ky8vT5L04IMP6vDhw/rJT35CYQEAAJJCVFgqKiqUnJwsp9OpUaNG6aWXXlK/fv2aHXvgwAGlp6f77Zs0aZI2bNggr9ermJiYZs/zeDzyeDy+bbfbLUnyer3yer1BSvIFZwcTnHmijN+/kSISc0diZoncN+cO9m2RbRrzhXvOL4vEzFJoc9/unA5jTFBvWd555x3V1dXpa1/7mv70pz/pBz/4gT788EOdPHlS3bt3bzL+a1/7mubOnatly5b59r333nv6xje+ofPnzyspKanZz+NyuZSTk9Nkf0FBgWJjY4MXCAAAhExdXZ0yMzNVW1ur+Pj4FscF/QpLRkaG77+HDBmiMWPGqH///nrjjTeUlZXV7DkOh8Nvu7FD3bz/y7Kzs/3mc7vdSklJUXp6equB78Rg196gzOOMMlo1okErDkfJ09BytnATibkjMbNE7ptzn3BNasNVhZ7X61VRUZHS0tJavBoebiIxsxTa3I33kNxKSO4S+rJ77rlHQ4YMUUVFRbPHExMTVVNT47fvwoULio6ObvaKTCOn0ymn09lkf0xMTNC/mJ764N7wehocQZ+zPYjE3JGYWSJ3owErCttwNXfmk9VTAj4nFLe7tovEzFJoct/ufCF/HRaPx6NTp061eNfOmDFjVFRU5LevsLBQI0aMiMgfBgAA0FTQC8uSJUtUUlKiyspKHTp0SNOnT5fb7dacOXMkfXFXzuzZs33j58+frzNnzigrK0unTp3S66+/rg0bNmjJkiXBXhoAAGingn6X0B//+EfNnDlTFy9e1H333afRo0fr4MGDSk1NlSRVV1erqqrKN75v377as2ePFi9erNdee03Jyclas2YNT2kGAAA+QS8sW7ZsafX4pk2bmuwbP368jh49GuylAACAMMF7CQEAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgvaAXltzcXD3yyCOKi4tTQkKCpk2bpvLy8lbPKS4ulsPhaPLx4YcfBnt5AACgHQp6YSkpKdGCBQt08OBBFRUV6caNG0pPT9fVq1dveW55ebmqq6t9HwMGDAj28gAAQDsUHewJf/Ob3/htb9y4UQkJCTpy5IjGjRvX6rkJCQnq2rVrsJcEAADauaAXlpvV1tZKkrp163bLscOGDdO1a9c0cOBALV++XBMnTmxxrMfjkcfj8W273W5Jktfrldfr/Yqr9ufsYIIzT5Tx+zdSRGLuSMwskTsccgdy+9k4Nti3uTaLxMxSaHPf7pwOY0zIfsOMMXr88cf1+eef63e/+12L48rLy1VaWqrhw4fL4/HozTff1Pr161VcXNziVRmXy6WcnJwm+wsKChQbGxu0DAAAIHTq6uqUmZmp2tpaxcfHtzgupIVlwYIF2r17t/bv36/7778/oHOnTp0qh8OhXbt2NXu8uSssKSkpunjxYquB78Rg196gzOOMMlo1okErDkfJ0+AIypztQSTmjsTMErnDIfcJ16TbHuv1elVUVKS0tDTFxMSEcFX2iMTMUmhzu91u9ejR45aFJWR3CT333HPatWuXSktLAy4rkjR69Gjl5+e3eNzpdMrpdDbZHxMTE/Qvpqc+uDdAngZH0OdsDyIxdyRmlsjdnt3J7WcobndtF4mZpdDkvt35gl5YjDF67rnntGPHDhUXF6tv3753NM8HH3ygpKSkIK8OAAC0R0EvLAsWLFBBQYHeeustxcXFqaamRpLUpUsXde7cWZKUnZ2tc+fOafPmzZKkvLw89enTR4MGDdL169eVn5+vbdu2adu2bcFeHgAAaIeCXljWrVsnSZowYYLf/o0bN2ru3LmSpOrqalVVVfmOXb9+XUuWLNG5c+fUuXNnDRo0SLt379bkyZODvTwAANAOheQuoVvZtGmT3/bSpUu1dOnSYC8FAACECd5LCAAAWI/CAgAArEdhAQAA1qOwAAAA61FYAACA9SgsAADAehQWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWC/p7CQEA2q8+L+y+7bHODkYvj5QGu/bKU+8I4apa98nqKW32uXH3cIUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKxHYQEAANajsAAAAOtRWAAAgPUoLAAAwHoUFgAAYD0KCwAAsB6FBQAAWC+6rRcAAMBX0eeF3Xftczk7GL08Uhrs2itPveOO5/lk9ZQgrioycIUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1QvYsof/8z//Uj3/8Y1VXV2vQoEHKy8vTY4891uL4kpISZWVl6eTJk0pOTtbSpUs1f/78UC0PAIA2czef2RQMjc+OakshucKydetWLVq0SC+++KI++OADPfbYY8rIyFBVVVWz4ysrKzV58mQ99thj+uCDD7Rs2TJ973vf07Zt20KxPAAA0M6EpLD89Kc/1T/90z/pmWee0YMPPqi8vDylpKRo3bp1zY5fv369evfurby8PD344IN65plnNG/ePP3kJz8JxfIAAEA7E/S7hK5fv64jR47ohRde8Nufnp6u9957r9lzDhw4oPT0dL99kyZN0oYNG+T1ehUTE9PkHI/HI4/H49uura2VJH322Wfyer1fNYaf6BtXgzNPg1FdXYOivVGqb7jzFxxqbyIxdyRmlshN7vAXiZmlP+f+9NNPm/2b/FVcvnxZkmSMaX0NQf2ski5evKj6+nr17NnTb3/Pnj1VU1PT7Dk1NTXNjr9x44YuXryopKSkJufk5uYqJyenyf6+fft+hdWHXmZbL6CNRGLuSMwskTvSRGLuSMwshT735cuX1aVLlxaPh+xBtw6Hf/M0xjTZd6vxze1vlJ2draysLN92Q0ODPvvsM3Xv3r3Vz9OW3G63UlJSdPbsWcXHx7f1cu6aSMwdiZklcpM7/EViZim0uY0xunz5spKTk1sdF/TC0qNHD3Xo0KHJ1ZQLFy40uYrSKDExsdnx0dHR6t69e7PnOJ1OOZ1Ov31du3a984XfRfHx8RH1g94oEnNHYmaJ3JEmEnNHYmYpdLlbu7LSKOgPuu3YsaOGDx+uoqIiv/1FRUUaO3Zss+eMGTOmyfjCwkKNGDEi6PeVAQCA9ickzxLKysrSL37xC73++us6deqUFi9erKqqKt/rqmRnZ2v27Nm+8fPnz9eZM2eUlZWlU6dO6fXXX9eGDRu0ZMmSUCwPAAC0MyF5DMuMGTP06aef6vvf/76qq6s1ePBg7dmzR6mpqZKk6upqv9dk6du3r/bs2aPFixfrtddeU3JystasWaMnn3wyFMtrM06nUytXrmxyV1a4i8TckZhZIje5w18kZpbsyO0wt3oeEQAAQBvjvYQAAID1KCwAAMB6FBYAAGA9CgsAALAehSXIcnNz9cgjjyguLk4JCQmaNm2aysvL/cYYY+RyuZScnKzOnTtrwoQJOnnyZButODjWrVunr3/9674XFRozZozeeecd3/FwzHyz3NxcORwOLVq0yLcvHHO7XC45HA6/j8TERN/xcMzc6Ny5c/r2t7+t7t27KzY2Vg899JCOHDniOx6O2fv06dPk++1wOLRgwQJJ4ZlZkm7cuKHly5erb9++6ty5s/r166fvf//7amho8I0Jx+yXL1/WokWLlJqaqs6dO2vs2LF6//33fcfbNLNBUE2aNMls3LjRnDhxwpSVlZkpU6aY3r17mytXrvjGrF692sTFxZlt27aZ48ePmxkzZpikpCTjdrvbcOVfza5du8zu3btNeXm5KS8vN8uWLTMxMTHmxIkTxpjwzPxlv//9702fPn3M17/+dbNw4ULf/nDMvXLlSjNo0CBTXV3t+7hw4YLveDhmNsaYzz77zKSmppq5c+eaQ4cOmcrKSrNv3z5z+vRp35hwzH7hwgW/73VRUZGRZN59911jTHhmNsaYH/zgB6Z79+7m7bffNpWVleZXv/qVuffee01eXp5vTDhmf/rpp83AgQNNSUmJqaioMCtXrjTx8fHmj3/8ozGmbTNTWELswoULRpIpKSkxxhjT0NBgEhMTzerVq31jrl27Zrp06WLWr1/fVssMib/4i78wv/jFL8I+8+XLl82AAQNMUVGRGT9+vK+whGvulStXmqFDhzZ7LFwzG2PM888/bx599NEWj4dz9i9buHCh6d+/v2loaAjrzFOmTDHz5s3z2/fEE0+Yb3/728aY8Px+19XVmQ4dOpi3337bb//QoUPNiy++2OaZuUsoxGprayVJ3bp1kyRVVlaqpqZG6enpvjFOp1Pjx4/Xe++91yZrDLb6+npt2bJFV69e1ZgxY8I+84IFCzRlyhT99V//td/+cM5dUVGh5ORk9e3bV9/61rf08ccfSwrvzLt27dKIESP01FNPKSEhQcOGDdPPf/5z3/Fwzt7o+vXrys/P17x58+RwOMI686OPPqr//d//1UcffSRJOnbsmPbv36/JkydLCs/v940bN1RfX69OnTr57e/cubP279/f5pkpLCFkjFFWVpYeffRRDR48WJJ8b/J48xtB9uzZs8kbQLY3x48f17333iun06n58+drx44dGjhwYFhn3rJli44eParc3Nwmx8I196hRo7R582bt3btXP//5z1VTU6OxY8fq008/DdvMkvTxxx9r3bp1GjBggPbu3av58+fre9/7njZv3iwpfL/fX7Zz505dunRJc+fOlRTemZ9//nnNnDlTDzzwgGJiYjRs2DAtWrRIM2fOlBSe2ePi4jRmzBitWrVK58+fV319vfLz83Xo0CFVV1e3eeaQvDQ/vvDd735Xf/jDH7R///4mxxwOh9+2MabJvvbmr/7qr1RWVqZLly5p27ZtmjNnjkpKSnzHwy3z2bNntXDhQhUWFjb5P5IvC7fcGRkZvv8eMmSIxowZo/79++uNN97Q6NGjJYVfZklqaGjQiBEj9NJLL0mShg0bppMnT2rdunV+740WjtkbbdiwQRkZGUpOTvbbH46Zt27dqvz8fBUUFGjQoEEqKyvTokWLlJycrDlz5vjGhVv2N998U/PmzVOvXr3UoUMHPfzww8rMzNTRo0d9Y9oqM1dYQuS5557Trl279O677+r+++/37W98NsXNbfTChQtNWmt707FjR/3lX/6lRowYodzcXA0dOlSvvvpq2GY+cuSILly4oOHDhys6OlrR0dEqKSnRmjVrFB0d7csWbrlvds8992jIkCGqqKgI2++1JCUlJWngwIF++x588EHf+6KFc3ZJOnPmjPbt26dnnnnGty+cM//7v/+7XnjhBX3rW9/SkCFDNGvWLC1evNh3NTVcs/fv318lJSW6cuWKzp49q9///vfyer3q27dvm2emsASZMUbf/e53tX37dv32t79V3759/Y43ftOLiop8+65fv66SkhKNHTv2bi83pIwx8ng8YZv5m9/8po4fP66ysjLfx4gRI/QP//APKisrU79+/cIy9808Ho9OnTqlpKSksP1eS9I3vvGNJi9R8NFHH/ne1DWcs0vSxo0blZCQoClTpvj2hXPmuro6RUX5/4ns0KGD72nN4Zxd+uJ/RJKSkvT5559r7969evzxx9s+c8gf1hth/vVf/9V06dLFFBcX+z0VsK6uzjdm9erVpkuXLmb79u3m+PHjZubMme3+qXDZ2dmmtLTUVFZWmj/84Q9m2bJlJioqyhQWFhpjwjNzc778LCFjwjP3v/3bv5ni4mLz8ccfm4MHD5q//du/NXFxceaTTz4xxoRnZmO+eOp6dHS0+eEPf2gqKirMf//3f5vY2FiTn5/vGxOu2evr603v3r3N888/3+RYuGaeM2eO6dWrl+9pzdu3bzc9evQwS5cu9Y0Jx+y/+c1vzDvvvGM+/vhjU1hYaIYOHWpGjhxprl+/boxp28wUliCT1OzHxo0bfWMaGhrMypUrTWJionE6nWbcuHHm+PHjbbfoIJg3b55JTU01HTt2NPfdd5/55je/6SsrxoRn5ubcXFjCMXfj6y7ExMSY5ORk88QTT5iTJ0/6jodj5ka//vWvzeDBg43T6TQPPPCA+a//+i+/4+Gafe/evUaSKS8vb3IsXDO73W6zcOFC07t3b9OpUyfTr18/8+KLLxqPx+MbE47Zt27davr162c6duxoEhMTzYIFC8ylS5d8x9sys8MYY0J/HQcAAODO8RgWAABgPQoLAACwHoUFAABYj8ICAACsR2EBAADWo7AAAADrUVgAAID1KCwAAMB6FBYAAGA9CgsAALAehQUAAFiPwgIAAKz3/wFDpkORlbnpUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train[\"Age\"].hist(bins=10, weights=np.ones_like(df_train[\"Age\"]) * 100. / len(df_train)).plot()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16ee2a",
   "metadata": {
    "id": "cd16ee2a"
   },
   "source": [
    "#### Q4.5: Report the age on the $99$-th quantile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107d0dd",
   "metadata": {
    "id": "d107d0dd"
   },
   "source": [
    "#### A4.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "550a44a1",
   "metadata": {
    "id": "550a44a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age at the 99th percentile: 74.0\n"
     ]
    }
   ],
   "source": [
    "#Hint: the \".quantile\" function of pandas will be enough\n",
    "# Calculate the age at the 99th percentile\n",
    "age_99th_percentile = df_train['Age'].quantile(0.99)\n",
    "print(\"Age at the 99th percentile:\", age_99th_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342406a0",
   "metadata": {
    "id": "342406a0"
   },
   "source": [
    "#### Q4.6: Given the previous analysis, would you say there are outliers in this dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f36f4",
   "metadata": {
    "id": "b99f36f4"
   },
   "source": [
    "#### A4.6: Given the histogram and the 99th percentile, the presence of individuals above 74 could be potentail outliers, if their age is signigicantly higher than 74. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc600672",
   "metadata": {
    "id": "fc600672"
   },
   "source": [
    "### Q5: Inspect the correlation of the numerical predictors with the target and report your findings (in training set). What does the correlation of \"Age\" tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9d34eed",
   "metadata": {
    "id": "f9d34eed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age    0.234037\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"Age\"]].corrwith(df_train[\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b23df8",
   "metadata": {
    "id": "76b23df8"
   },
   "source": [
    "### Q6: Let us apply Linear Regression for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454ebd7",
   "metadata": {
    "id": "c454ebd7"
   },
   "source": [
    "#### Q6.1: Discuss why Linear Regression is not immediately applicable in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980b7a0",
   "metadata": {
    "id": "2980b7a0"
   },
   "source": [
    "#### A6.1: Linear regression is designed for regression problems where the outcome is a continuous varaiable. However, the target variable here is a binary, making is a classification problem. Also we have rows including null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bc7bb",
   "metadata": {
    "id": "de9bc7bb"
   },
   "source": [
    "#### Q6.2: Drop rows that include NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa329364",
   "metadata": {
    "id": "aa329364"
   },
   "source": [
    "#### A6.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d8b2cd6",
   "metadata": {
    "id": "7d8b2cd6"
   },
   "outputs": [],
   "source": [
    "df_train_dropna = df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efdf8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows with at least one NaN in Training Set: 0\n"
     ]
    }
   ],
   "source": [
    "rows_with_nan_train_dropna = df_train_dropna.isnull().any(axis=1).sum()\n",
    "print(\"Number of Rows with at least one NaN in Training Set:\", rows_with_nan_train_dropna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c81fea",
   "metadata": {
    "id": "f1c81fea"
   },
   "source": [
    "#### Q6.3: Train a linear regression model on the training set. Use the numeric columns only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f44441",
   "metadata": {
    "id": "c8f44441"
   },
   "source": [
    "#### A6.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c34392c7",
   "metadata": {
    "id": "c34392c7"
   },
   "outputs": [],
   "source": [
    "#Step 1 -- take the numeric columns\n",
    "numericals = [] #start with an empty array\n",
    "for i in range(len(df_train_dropna.dtypes)): #for all columns\n",
    "    coltype = df_train_dropna.dtypes[i] #take the type of column\n",
    "    if np.issubdtype(coltype,np.number): #check if the type is numberic\n",
    "        numericals.append(df_train_dropna.columns[i]) # append the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de03194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'fnlwgt', 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week', 'Target']\n"
     ]
    }
   ],
   "source": [
    "print(numericals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2455e0ad",
   "metadata": {
    "id": "2455e0ad"
   },
   "outputs": [],
   "source": [
    "#Step 2 -- fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = df_train_dropna[numericals] #training predictors\n",
    "y = df_train_dropna['Target'] #training target\n",
    "clf = LinearRegression().fit(X, y) #time to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180084ec",
   "metadata": {
    "id": "180084ec"
   },
   "source": [
    "#### Q6.4: Apply the linear model that we just fit and predict the training target. Show the first couple of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755db00",
   "metadata": {
    "id": "9755db00"
   },
   "source": [
    "#### A6.4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebd7fda5",
   "metadata": {
    "id": "ebd7fda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions: [-7.38951414e-16  1.83099136e-15  2.45719546e-15  2.42345710e-15\n",
      "  1.79528186e-15] mean 0.24892248524633645\n"
     ]
    }
   ],
   "source": [
    "#use \"clf.predict\" function\n",
    "# Predict the training target\n",
    "train_predictions = clf.predict(X)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions:\", train_predictions[:5], \"mean\",train_predictions.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668af8f6",
   "metadata": {
    "id": "668af8f6"
   },
   "source": [
    "#### Q6.5: To make a classification on the *training set*, use a cutoff value = 0.4 so that if the output is more than this value then we classify as \"1\". Compute the accuracy, sensitivity, and specificity manually (do not use a function). Discuss the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad6985",
   "metadata": {
    "id": "19ad6985"
   },
   "source": [
    "#### A6.5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ad0caea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.38951414e-16  1.83099136e-15  2.45719546e-15 ...  2.79063877e-15\n",
      "  1.60511007e-15  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04eb793b",
   "metadata": {
    "id": "04eb793b"
   },
   "outputs": [],
   "source": [
    "cutoff = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1703d10",
   "metadata": {
    "id": "f1703d10"
   },
   "outputs": [],
   "source": [
    "# Convert predictions to binary using the cutoff\n",
    "binary_predictions = [1 if x > cutoff else 0 for x in train_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives = sum((y == 1) & (np.array(binary_predictions) == 1))\n",
    "true_negatives = sum((y == 0) & (np.array(binary_predictions) == 0))\n",
    "false_positives = sum((y == 0) & (np.array(binary_predictions) == 1))\n",
    "false_negatives = sum((y == 1) & (np.array(binary_predictions) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_train = (true_positives + true_negatives) / len(y)\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_train = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_train = true_negatives / (true_negatives + false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64698f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives=7508, true_negatives=22654, false_positives=0, false_negatives=0\n"
     ]
    }
   ],
   "source": [
    "print(f'true_positives={true_positives}, true_negatives={true_negatives}, false_positives={false_positives}, false_negatives={false_negatives}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7e38902",
   "metadata": {
    "id": "c7e38902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", round(accuracy_train,3), \"\\nSensitivity:\", round(sensitivity_train,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_train,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84826259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "binary_predictions_series = pd.Series(binary_predictions)\n",
    "print(binary_predictions_series.quantile(0.99))\n",
    "#print(binary_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e794603",
   "metadata": {
    "id": "3e794603"
   },
   "source": [
    "#### Q6.6: Apply the same on training set but decrease the cutoff down to 0.2. Inspect the metrics we looked before, and compare the results with the cutoff 0.4. Interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3104cd",
   "metadata": {
    "id": "6d3104cd"
   },
   "source": [
    "#### A6.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba0f4b4f",
   "metadata": {
    "id": "ba0f4b4f"
   },
   "outputs": [],
   "source": [
    "cutoff_altenative = 0.2 #cutoff value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ded638e2",
   "metadata": {
    "id": "ded638e2"
   },
   "outputs": [],
   "source": [
    "# Convert predictions to binary using the cutoff\n",
    "binary_predictions_alt = [1 if x > cutoff_altenative else 0 for x in train_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives_alt = sum((y == 1) & (np.array(binary_predictions_alt) == 1))\n",
    "true_negatives_alt = sum((y == 0) & (np.array(binary_predictions_alt) == 0))\n",
    "false_positives_alt = sum((y == 0) & (np.array(binary_predictions_alt) == 1))\n",
    "false_negatives_alt = sum((y == 1) & (np.array(binary_predictions_alt) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_train_alt = (true_positives_alt + true_negatives_alt) / len(y)\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_train_alt = true_positives_alt / (true_positives_alt + false_negatives_alt)\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_train_alt = true_negatives_alt / (true_negatives_alt + false_positives_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "541894f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives_alt=7508, true_negatives_alt=22654, false_positives_alt=0, false_negatives_alt=0\n"
     ]
    }
   ],
   "source": [
    "print(f'true_positives_alt={true_positives_alt}, true_negatives_alt={true_negatives_alt}, false_positives_alt={false_positives_alt}, false_negatives_alt={false_negatives_alt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "985e2d17",
   "metadata": {
    "id": "985e2d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", round(accuracy_train_alt,3), \"\\nSensitivity:\", round(sensitivity_train_alt,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_train_alt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d975409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives_alt=7508, true_negatives_alt=22654, false_positives_alt=0, false_negatives_alt=0\n",
      "Accuracy: 1.0 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "cutoff_altenative = 0.05 #cutoff value\n",
    "\n",
    "# Convert predictions to binary using the cutoff\n",
    "binary_predictions_alt = [1 if x > cutoff_altenative else 0 for x in train_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives_alt = sum((y == 1) & (np.array(binary_predictions_alt) == 1))\n",
    "true_negatives_alt = sum((y == 0) & (np.array(binary_predictions_alt) == 0))\n",
    "false_positives_alt = sum((y == 0) & (np.array(binary_predictions_alt) == 1))\n",
    "false_negatives_alt = sum((y == 1) & (np.array(binary_predictions_alt) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_train_alt = (true_positives_alt + true_negatives_alt) / len(y)\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_train_alt = true_positives_alt / (true_positives_alt + false_negatives_alt)\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_train_alt = true_negatives_alt / (true_negatives_alt + false_positives_alt)\n",
    "\n",
    "print(f'true_positives_alt={true_positives_alt}, true_negatives_alt={true_negatives_alt}, false_positives_alt={false_positives_alt}, false_negatives_alt={false_negatives_alt}')\n",
    "print(\"Accuracy:\", round(accuracy_train_alt,3), \"\\nSensitivity:\", round(sensitivity_train_alt,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_train_alt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6064d1",
   "metadata": {
    "id": "2e6064d1"
   },
   "source": [
    "#### Q6.7: Now use the linear model on the *test set*. Choose the cutoff value = 0.4 and return the accuracy, sensitivity, and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ec97f",
   "metadata": {
    "id": "186ec97f"
   },
   "source": [
    "#### A6.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "027e8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions: [-7.38951414e-16  1.83099136e-15  2.45719546e-15  2.42345710e-15\n",
      "  1.79528186e-15] mean 0.2456839309428951\n",
      "true_positives_test=3700, true_negatives_test=11360, false_positives_test=0, false_negatives_test=0\n",
      "Accuracy: 0.499 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "df_test_dropna = df_test.dropna()\n",
    "\n",
    "#Step 2 -- fit\n",
    "X_test = df_test_dropna[numericals] #training predictors\n",
    "y_test = df_test_dropna['Target'] #training target\n",
    "clf_test = LinearRegression().fit(X_test, y_test) #time to fit\n",
    "\n",
    "#use \"clf.predict\" function\n",
    "# Predict the training target\n",
    "test_predictions = clf_test.predict(X_test)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions:\", train_predictions[:5], \"mean\",test_predictions.mean())\n",
    "\n",
    "cutoff = 0.4\n",
    "\n",
    "# Convert predictions to binary using the cutoff\n",
    "binary_predictions_test = [1 if x > cutoff else 0 for x in test_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives_test = sum((y_test == 1) & (np.array(binary_predictions_test) == 1))\n",
    "true_negatives_test = sum((y_test == 0) & (np.array(binary_predictions_test) == 0))\n",
    "false_positives_test = sum((y_test == 0) & (np.array(binary_predictions_test) == 1))\n",
    "false_negatives_test = sum((y_test == 1) & (np.array(binary_predictions_test) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_test = (true_positives_test + true_negatives_test) / len(y)\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_test = true_positives_test / (true_positives_test + false_negatives_test)\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_test = true_negatives_test / (true_negatives_test + false_positives_test)\n",
    "\n",
    "print(f'true_positives_test={true_positives_test}, true_negatives_test={true_negatives_test}, false_positives_test={false_positives_test}, false_negatives_test={false_negatives_test}')\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_test,3), \"\\nSensitivity:\", round(sensitivity_test,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80415e41",
   "metadata": {
    "id": "80415e41"
   },
   "source": [
    "#### Q6.8: Try different cutoff values on the training set and choose the best one according to the performance on the training set. Choose the following metric to optimize: *keep accuracy above 76% and trying to obtain the best possible sensitivity*. What is the optimal cutoff value in your experiment? (Note: this approach will be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6b24a",
   "metadata": {
    "id": "65a6b24a"
   },
   "source": [
    "#### A6.8: \n",
    "- Application of the previous chunks in an iterated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d99e09d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff: 0.1, Highest Sensitivity: 1.0\n"
     ]
    }
   ],
   "source": [
    "optimal_cutoff = None\n",
    "highest_sensitivity = 0\n",
    "\n",
    "for cutoff in np.arange(0.1, 0.9, 0.01):\n",
    "    binary_predictions = [1 if x > cutoff else 0 for x in train_predictions]\n",
    "    true_positives = sum((y == 1) & (np.array(binary_predictions) == 1))\n",
    "    true_negatives = sum((y == 0) & (np.array(binary_predictions) == 0))\n",
    "    false_positives = sum((y == 0) & (np.array(binary_predictions) == 1))\n",
    "    false_negatives = sum((y == 1) & (np.array(binary_predictions) == 0))\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / len(y)\n",
    "    sensitivity = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if accuracy >= 0.76 and sensitivity > highest_sensitivity:\n",
    "        highest_sensitivity = sensitivity\n",
    "        optimal_cutoff = cutoff\n",
    "\n",
    "print(f\"Optimal Cutoff: {optimal_cutoff}, Highest Sensitivity: {highest_sensitivity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835660a",
   "metadata": {
    "id": "9835660a"
   },
   "source": [
    "#### Q6.9: In the previous question we tuned the cutoff value on the training set. This initialy makes sense, because we cannot tune on the test-set, which will be indirectly training on the test set. However, we also discussed in the validation-set approach that it is not a good practice to *compare* models on the training set, because of a natural bias in this procedure. Hence, it is a better approach to tune a value on a validation set. For this purpose, apply the following steps:\n",
    "- Split the training set as 80% (training) - 20% (validation) sets. You can take the first 80% and last 20% and no need to randomize the selection.\n",
    "- Train a linear model on the training set obtained in the previous approach. Training one model is enough.\n",
    "- Compare cutoff values between 0.2 - 0.8 with 0.05 increments. Pick the best model by looking at the validation set, where the \"best\" model has an accuracy over 76% and has the highest sensitivity still.\n",
    "- Test the validated cutoff on the test set and return the metrics.\n",
    "- [Extra / optional] Instead of the hold-out validation that you just applied, try 5-fold cross validation in the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d29000",
   "metadata": {
    "id": "05d29000"
   },
   "source": [
    "#### A6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fc3b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Split the Training Set\n",
    "split_index = int(len(df_train_dropna) * 0.8)\n",
    "training_set = df_train_dropna[:split_index]\n",
    "validation_set = df_train_dropna[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76e55681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Train a Linear Model\n",
    "\n",
    "X_train = training_set[numericals]\n",
    "y_train = training_set['Target']\n",
    "clf = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6942b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 and 4: Compare Cutoff Values and Pick the Best Model\n",
    "\n",
    "best_cutoff = 0.2\n",
    "best_sensitivity = 0\n",
    "\n",
    "for cutoff in np.arange(0.2, 0.85, 0.05):\n",
    "    validation_predictions = clf.predict(validation_set[numericals])\n",
    "    binary_predictions = [1 if x > cutoff else 0 for x in validation_predictions]\n",
    "\n",
    "    # Calculate metrics...\n",
    "    # Similar to previous steps but using validation_set\n",
    "\n",
    "    if accuracy >= 0.76 and sensitivity > best_sensitivity:\n",
    "        best_sensitivity = sensitivity\n",
    "        best_cutoff = cutoff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8fb3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Test the Validated Cutoff on the Test Set\n",
    "test_predictions = clf.predict(df_test[numericals])\n",
    "binary_test_predictions = [1 if x > best_cutoff else 0 for x in test_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c00bebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff: 0.2, Highest Average Sensitivity: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Step 6: [Optional] 5-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "best_cutoff = None\n",
    "highest_sensitivity = 0\n",
    "\n",
    "for cutoff in np.arange(0.2, 0.85, 0.05):\n",
    "    sensitivities = []\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(df_train_dropna):\n",
    "        # Split data into training and validation sets\n",
    "        X_train_fold = df_train_dropna.iloc[train_index][numericals]\n",
    "        y_train_fold = df_train_dropna.iloc[train_index]['Target']\n",
    "        X_val_fold = df_train_dropna.iloc[val_index][numericals]\n",
    "        y_val_fold = df_train_dropna.iloc[val_index]['Target']\n",
    "\n",
    "        # Train the model\n",
    "        clf_fold = LinearRegression().fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        val_predictions = clf_fold.predict(X_val_fold)\n",
    "        binary_val_predictions = [1 if x > cutoff else 0 for x in val_predictions]\n",
    "\n",
    "        # Calculate accuracy and sensitivity\n",
    "        true_positives = sum((y_val_fold == 1) & (np.array(binary_val_predictions) == 1))\n",
    "        true_negatives = sum((y_val_fold == 0) & (np.array(binary_val_predictions) == 0))\n",
    "        false_positives = sum((y_val_fold == 0) & (np.array(binary_val_predictions) == 1))\n",
    "        false_negatives = sum((y_val_fold == 1) & (np.array(binary_val_predictions) == 0))\n",
    "\n",
    "        accuracy = accuracy_score(y_val_fold, binary_val_predictions)\n",
    "        sensitivity = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Average metrics across folds\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    # Update best cutoff if conditions are met\n",
    "    if avg_accuracy >= 0.76 and avg_sensitivity > highest_sensitivity:\n",
    "        highest_sensitivity = avg_sensitivity\n",
    "        best_cutoff = cutoff\n",
    "\n",
    "print(f\"Optimal Cutoff: {best_cutoff}, Highest Average Sensitivity: {highest_sensitivity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86089a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da7d7823",
   "metadata": {
    "id": "da7d7823"
   },
   "source": [
    "#### Q6.10: Previously we have seen that if we decrese the cutoff value then we get more sensitivity as we try to classify more \"1\"s. Obviously we would expect to have sensitivity = 1 if we pick the smallest possible cutoff. To this end, try a cutoff = 0 on the test set and report the sensitivity. If the sensitivity is not equal to 1, please give a possible reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f848a0",
   "metadata": {
    "id": "f9f848a0"
   },
   "source": [
    "#### A6.10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3086ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions: [-7.38951414e-16  1.83099136e-15  2.45719546e-15  2.42345710e-15\n",
      "  1.79528186e-15] mean 0.2456839309428951\n",
      "true_positives_test=3700, true_negatives_test=1239, false_positives_test=10121, false_negatives_test=0\n",
      "Accuracy: 0.164 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 0.109\n"
     ]
    }
   ],
   "source": [
    "df_test_dropna = df_test.dropna()\n",
    "\n",
    "#Step 2 -- fit\n",
    "X_test = df_test_dropna[numericals] #training predictors\n",
    "y_test = df_test_dropna['Target'] #training target\n",
    "clf_test = LinearRegression().fit(X_test, y_test) #time to fit\n",
    "\n",
    "#use \"clf.predict\" function\n",
    "# Predict the training target\n",
    "test_predictions = clf_test.predict(X_test)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions:\", train_predictions[:5], \"mean\",test_predictions.mean())\n",
    "cutoff = 0\n",
    "\n",
    "# Convert predictions to binary using the cutoff\n",
    "binary_predictions_test = [1 if x > cutoff else 0 for x in test_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives_test = sum((y_test == 1) & (np.array(binary_predictions_test) == 1))\n",
    "true_negatives_test = sum((y_test == 0) & (np.array(binary_predictions_test) == 0))\n",
    "false_positives_test = sum((y_test == 0) & (np.array(binary_predictions_test) == 1))\n",
    "false_negatives_test = sum((y_test == 1) & (np.array(binary_predictions_test) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_test = (true_positives_test + true_negatives_test) / len(y)\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_test = true_positives_test / (true_positives_test + false_negatives_test)\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_test = true_negatives_test / (true_negatives_test + false_positives_test)\n",
    "\n",
    "print(f'true_positives_test={true_positives_test}, true_negatives_test={true_negatives_test}, false_positives_test={false_positives_test}, false_negatives_test={false_negatives_test}')\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_test,3), \"\\nSensitivity:\", round(sensitivity_test,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fba4f2",
   "metadata": {
    "id": "e3fba4f2"
   },
   "source": [
    "#### Q6.11 [Bonus / Optional]: Encode \"worksclass\" with dummies so that we can include these variables in the linear regression (i.e., introduce a column for each value the workclass can take and write '1' if this is true -- note that you should drop one of these dummies without loss of generality as from the others you can figure out the one left out). Train the linear regression by using this new variable in addition to the previous variables. Test on the test set by using the cutoff value you found previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755d39f",
   "metadata": {
    "id": "a755d39f"
   },
   "source": [
    "#### A6.11:\n",
    "- One can iterate over the \".dtypes\" of the training set and for every \"O\" data type, convert this to \"Categorical\". Then, using \"pd.get_dummies\" one can obtain dummies on this categorical variable. The rest is identical to what we did before -- only add the new additional numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87feb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Convert Categorical Variables to Dummies\n",
    "\n",
    "# For the training set\n",
    "df_train_dummies = pd.get_dummies(df_train, columns=['Workclass'], drop_first=True)\n",
    "\n",
    "# For the test set\n",
    "df_test_dummies = pd.get_dummies(df_test, columns=['Workclass'], drop_first=True)\n",
    "\n",
    "# Ensure that both training and test sets have the same dummy columns\n",
    "df_train_dummies, df_test_dummies = df_train_dummies.align(df_test_dummies, join='inner', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c42914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Prepare the Data\n",
    "\n",
    "# Assuming 'numericals' contains the names of your numeric predictor columns\n",
    "X_train = df_train_dummies[numericals + list(df_train_dummies.columns[df_train_dummies.columns.str.startswith('Workclass_')])]\n",
    "y_train = df_train_dummies['Target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "524ca95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Train the Linear Regression Model\n",
    "\n",
    "clf = LinearRegression().fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9271a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives_test=17, true_negatives_test=0, false_positives_test=0, false_negatives_test=0\n",
      "Accuracy: 0.001 \n",
      "Sensitivity: 1.0 \n",
      "Specificity: 0\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Test on the Test Set\n",
    "\n",
    "X_test = df_test_dummies[numericals + list(df_test_dummies.columns[df_test_dummies.columns.str.startswith('Workclass_')])]\n",
    "y_test = df_test_dummies['Target']\n",
    "\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# Apply the previously determined optimal cutoff\n",
    "optimal_cutoff = 0.2 # (set this to the value found previously)\n",
    "binary_test_predictions = [1 if x > optimal_cutoff else 0 for x in test_predictions]\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "true_positives_test = sum((y_test == 1) & (np.array(test_predictions) == 1))\n",
    "true_negatives_test = sum((y_test == 0) & (np.array(test_predictions) == 0))\n",
    "false_positives_test = sum((y_test == 0) & (np.array(test_predictions) == 1))\n",
    "false_negatives_test = sum((y_test == 1) & (np.array(test_predictions) == 0))\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy_test = (true_positives_test + true_negatives_test) / len(y) if len(y_test) > 0 else 0\n",
    "\n",
    "# Sensitivity: TP / (TP + FN)\n",
    "sensitivity_test = true_positives_test / (true_positives_test + false_negatives_test) if (true_positives_test + false_negatives_test) > 0 else 0\n",
    "\n",
    "# Specificity: TN / (TN + FP)\n",
    "specificity_test = true_negatives_test / (true_negatives_test + false_positives_test) if (true_negatives_test + false_positives_test) > 0 else 0\n",
    "\n",
    "print(f'true_positives_test={true_positives_test}, true_negatives_test={true_negatives_test}, false_positives_test={false_positives_test}, false_negatives_test={false_negatives_test}')\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_test,3), \"\\nSensitivity:\", round(sensitivity_test,3),\\\n",
    "      \"\\nSpecificity:\", round(specificity_test,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d35d5",
   "metadata": {
    "id": "501d35d5"
   },
   "source": [
    "### Final notes\n",
    "Note that linear regression is not designed for such tasks, however, it works great in many cases. A variant of linear regression, that is called the *logistic regression*, takes a linear relationship as in the linear regression setting, but applies a non-linear mapping on the output. In return, we obtain predictions that are between [0,1], hence simply using \"predict\" function would classify the instances with a prediction of >= 0.5 as \"1\". We can similarly change the threshold. Although logistic regression will be introduced in more detail, we can also write similar code to make use of it easily as the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de14c84f",
   "metadata": {
    "id": "de14c84f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X, y) \n",
    "#X,y have to be filled before so that X comprises numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9cc8f75",
   "metadata": {
    "id": "a9cc8f75"
   },
   "outputs": [],
   "source": [
    "scores_linear_training = clf.predict(X) #predict function\n",
    "# alternatively, for using a cutoff, first get the assigned 'probabilities' by using \"probs = clf.predict_proba(X)\"\n",
    "# and then apply thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af32967",
   "metadata": {
    "id": "3af32967"
   },
   "source": [
    "As extra, you can try coding other concepts such as:\n",
    "- Visualising lift charts and comparing models via lift charts.\n",
    "- Applying more advanced algorithms and/or using further categorial variables by using several encodings.\n",
    "- Analyzing the categorical variables further to see if we have an ordinal relationship that can be modeled via integers.\n",
    "- As mentioned above, using K fold cross validation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "797ae857",
    "797ec48d",
    "146b35ef",
    "23e9f381",
    "5adcc5a4",
    "f49afb50",
    "61367d5f",
    "68233a89",
    "d3366487",
    "3d79e4f3",
    "a87bc8cc",
    "3e650ccc",
    "7e568da8",
    "f9ea66e1",
    "e7656c1e",
    "cfb01a85",
    "92e8b941",
    "89f1c7f0",
    "2018caa6",
    "0c0cb992",
    "58fcae06",
    "13ab38b5",
    "3a5ca4f6",
    "cd16ee2a",
    "d107d0dd",
    "342406a0",
    "b99f36f4",
    "c454ebd7",
    "2980b7a0",
    "de9bc7bb",
    "aa329364",
    "f1c81fea",
    "c8f44441",
    "180084ec",
    "9755db00",
    "668af8f6",
    "19ad6985",
    "3e794603",
    "2e6064d1",
    "186ec97f",
    "80415e41",
    "65a6b24a",
    "9835660a",
    "05d29000",
    "da7d7823",
    "f9f848a0",
    "e3fba4f2",
    "a755d39f",
    "501d35d5"
   ],
   "name": "Project Questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
