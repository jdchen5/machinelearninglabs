{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdchen5/machinelearninglabs/blob/main/W22/requiredActivity22-3-multiTask-JC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "AyXFDj5z5OsF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyXFDj5z5OsF",
        "outputId": "40138e2b-5f31-4b6b-ce79-c20e7ed987e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEgNzGSD7pps",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEgNzGSD7pps",
        "outputId": "5b2131ef-0de5-459e-e740-f711d6247167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S55oDpGBLlv7",
        "outputId": "7b157476-f8e2-406f-a171-6e4da669a24e"
      },
      "id": "S55oDpGBLlv7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only required at the beinning to split the traing and set data"
      ],
      "metadata": {
        "id": "nRb6cqktKif-"
      },
      "id": "nRb6cqktKif-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_miGbcPG7uks",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_miGbcPG7uks",
        "outputId": "2197ce6f-9fe9-4f2f-9783-36f48106f305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['choon', 'ch4f', 'karyadi', 'bpm', 'an2i', 'glickman', 'at33', 'danieln', 'boland', 'cheyer', 'night', 'saavik', 'mitchell', 'kk49', 'steffi', 'kawamura', 'megak', 'sz24', 'phoebe', 'tammo']\n",
            "Data split into train and test directories.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to the dataset\n",
        "data_dir = '/content/gdrive/My Drive/Pythoncode/W22/faces_4'\n",
        "# Get all class directories\n",
        "class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "print(class_dirs)\n",
        "\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Split each class's files and move them to the corresponding train/test directories\n",
        "for class_dir in class_dirs:\n",
        "    # Full path to the class directory\n",
        "    class_path = os.path.join(data_dir, class_dir)\n",
        "    # Get all files in the class directory\n",
        "    files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "    # Check if there are any files to split\n",
        "    if not files:\n",
        "        print(f\"No files to split in directory {class_dir}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Split the files into 80% train and 20% test\n",
        "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create corresponding class directories in train and test directories\n",
        "    train_class_dir = os.path.join(train_dir, class_dir)\n",
        "    test_class_dir = os.path.join(test_dir, class_dir)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Function to copy files to the specified directory\n",
        "    def copy_files(files, source_dir, destination_dir):\n",
        "        for file in files:\n",
        "            shutil.copy(os.path.join(source_dir, file), os.path.join(destination_dir, file))\n",
        "\n",
        "    # Copy the files to their respective directories\n",
        "    copy_files(train_files, class_path, train_class_dir)\n",
        "    copy_files(test_files, class_path, test_class_dir)\n",
        "\n",
        "print(\"Data split into train and test directories.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c7g5BYQJWYAl",
      "metadata": {
        "id": "c7g5BYQJWYAl"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "26d377b5",
      "metadata": {
        "id": "26d377b5"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3419b75b",
      "metadata": {
        "id": "3419b75b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img = cv2.imread(\"some_image.pgm\", cv2.IMREAD_COLOR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "FMSlLxBruSTp",
      "metadata": {
        "id": "FMSlLxBruSTp"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e74202bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74202bf",
        "outputId": "31fca7c2-2613-4b05-a562-09f92e79d8a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an2i',\n",
              " 'at33',\n",
              " 'boland',\n",
              " 'bpm',\n",
              " 'ch4f',\n",
              " 'cheyer',\n",
              " 'choon',\n",
              " 'danieln',\n",
              " 'glickman',\n",
              " 'karyadi',\n",
              " 'kawamura',\n",
              " 'kk49',\n",
              " 'megak',\n",
              " 'mitchell',\n",
              " 'night',\n",
              " 'phoebe',\n",
              " 'saavik',\n",
              " 'steffi',\n",
              " 'sz24',\n",
              " 'tammo',\n",
              " 'test',\n",
              " 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Custom helper function to load images in PGM format using OpenCV\n",
        "def img_loader(path):\n",
        "    return cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "data = ImageFolder(root='/content/gdrive/My Drive/Pythoncode/W22/faces_4/', loader=img_loader, transform=transforms)\n",
        "data.classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#custom class for face and expression\n",
        "class CMUFaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.person_labels = set()\n",
        "        self.expression_labels = set()\n",
        "\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pgm'):\n",
        "                    img_path = os.path.join(subdir, file)\n",
        "                    self.images.append(img_path)\n",
        "\n",
        "                    # Extract and store person and expression labels\n",
        "                    person_label, expression_label = self._extract_labels(img_path)\n",
        "                    self.person_labels.add(person_label)\n",
        "                    self.expression_labels.add(expression_label)\n",
        "\n",
        "        # Convert sets to sorted lists for consistent indexing\n",
        "        self.person_labels = sorted(list(self.person_labels))\n",
        "        self.expression_labels = sorted(list(self.expression_labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        person_label, expression_label = self._extract_labels(img_path)\n",
        "        person_idx = self.person_labels.index(person_label)\n",
        "        expression_idx = self.expression_labels.index(expression_label)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, (person_idx, expression_idx)\n",
        "\n",
        "    def _extract_labels(self, img_path):\n",
        "        \"\"\"\n",
        "        Extracts person and expression labels from an image file path.\n",
        "        \"\"\"\n",
        "        basename = os.path.basename(img_path)\n",
        "        parts = basename.split('_')\n",
        "        person_label = parts[0]\n",
        "        expression_label = parts[2]  # Adjust index based on your file naming convention\n",
        "        return person_label, expression_label\n"
      ],
      "metadata": {
        "id": "-Cpj_AECLJBG"
      },
      "id": "-Cpj_AECLJBG",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "xp83mM171IiJ",
      "metadata": {
        "id": "xp83mM171IiJ"
      },
      "outputs": [],
      "source": [
        "# Get some random dataset  images\n",
        "\n",
        "def show_test_images(test_dataset, test_loader):\n",
        "    # Get some random test images\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels_tuple = next(dataiter)\n",
        "    person_labels, expression_labels = labels_tuple\n",
        "\n",
        "    # Show images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "    # Get the class names for persons and expressions from the test_dataset\n",
        "    person_class_names = test_dataset.person_labels\n",
        "    expression_class_names = test_dataset.expression_labels\n",
        "\n",
        "    # Prepare labels for printing\n",
        "    person_labels_text = ' '.join('%5s' % person_class_names[person_labels[j]] for j in range(len(person_labels)))\n",
        "    expression_labels_text = ' '.join('%5s' % expression_class_names[expression_labels[j]] for j in range(len(expression_labels)))\n",
        "\n",
        "    # Print labels with class names\n",
        "    print('GroundTruth Persons: ', person_labels_text)\n",
        "    print('GroundTruth Expressions: ', expression_labels_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "47cfee25",
      "metadata": {
        "id": "47cfee25"
      },
      "outputs": [],
      "source": [
        "# Define a CNN architecture inspired by LeNet5, adjusted for multi-tasks\n",
        "class MultiTaskLeNet5(nn.Module):\n",
        "    def __init__(self, num_classes_taskA, num_classes_taskB):\n",
        "        super(MultiTaskLeNet5, self).__init__()\n",
        "        # Shared layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)  # Adjust the input channels as needed\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # Task A and B specific layers (initialized later)\n",
        "        self.fc1_taskA = None\n",
        "        self.fc2_taskA = nn.Linear(120, 84)\n",
        "        self.fc3_taskA = nn.Linear(84, num_classes_taskA)\n",
        "\n",
        "        self.fc1_taskB = None\n",
        "        self.fc2_taskB = nn.Linear(120, 50)\n",
        "        self.fc3_taskB = nn.Linear(50, num_classes_taskB)\n",
        "\n",
        "        # Flag to check if dynamic layers are initialized\n",
        "        self.initialized = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Dynamically calculate the number of flat features\n",
        "        num_flat_features = x.nelement() / x.shape[0]  # This calculates the total number of features per sample\n",
        "\n",
        "        if not self.initialized:\n",
        "            # Now that we have the number of flat features, initialize the fc layers properly\n",
        "            self.fc1_taskA = nn.Linear(int(num_flat_features), 120).to(x.device)\n",
        "            self.fc1_taskB = nn.Linear(int(num_flat_features), 120).to(x.device)\n",
        "            self.fc2_taskA = nn.Linear(120, 84).to(x.device)  # Re-initialize to ensure it's on the correct device\n",
        "            self.fc2_taskB = nn.Linear(120, 50).to(x.device)  # Re-initialize\n",
        "            self.fc3_taskA = nn.Linear(84, self.fc3_taskA.out_features).to(x.device)  # Assuming out_features is set\n",
        "            self.fc3_taskB = nn.Linear(50, self.fc3_taskB.out_features).to(x.device)  # Assuming out_features is set\n",
        "            self.initialized = True\n",
        "\n",
        "        # Flatten the features for the fully connected layers\n",
        "        x = x.view(-1, int(num_flat_features))\n",
        "\n",
        "        # Task A path\n",
        "        x_a = F.relu(self.fc1_taskA(x))\n",
        "        x_a = F.relu(self.fc2_taskA(x_a))\n",
        "        x_a = self.fc3_taskA(x_a)\n",
        "\n",
        "        # Task B path\n",
        "        x_b = F.relu(self.fc1_taskB(x))\n",
        "        x_b = F.relu(self.fc2_taskB(x_b))\n",
        "        x_b = self.fc3_taskB(x_b)\n",
        "\n",
        "        return x_a, x_b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PSoJAGL8Zy_x",
      "metadata": {
        "id": "PSoJAGL8Zy_x"
      },
      "outputs": [],
      "source": [
        "# Define a function to reset the model\n",
        "def reset_model(num_classes, device):\n",
        "    model = Net(num_classes=num_classes).to(device)\n",
        "    return model\n",
        "\n",
        "# Define a function to reset the optimizer\n",
        "def reset_optimizer(model, lr=0.01, momentum=0.9):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "RMXn4wl25z8k",
      "metadata": {
        "id": "RMXn4wl25z8k"
      },
      "outputs": [],
      "source": [
        "# Train and Test function\n",
        "def train(model, device, train_loader, optimizer, criterion1, criterion2, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, (targetsA, targetsB)) in enumerate(train_loader):\n",
        "        data, targetsA, targetsB = data.to(device), targetsA.to(device), targetsB.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputsA, outputsB = model(data)\n",
        "        lossA = criterion1(outputsA, targetsA)\n",
        "        lossB = criterion2(outputsB, targetsB)\n",
        "        loss = lossA + lossB  # Combine losses; adjust if you're weighting tasks differently\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:  # Adjust log interval as needed\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def test(model, device, test_loader, criterion1, criterion2):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target1, target2 = data.to(device), target[0].to(device), target[1].to(device)\n",
        "            output1, output2 = model(data)\n",
        "            test_loss += (criterion1(output1, target1) + criterion2(output2, target2)).item()  # Sum up batch loss\n",
        "            pred1 = output1.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct1 += pred1.eq(target1.view_as(pred1)).sum().item()\n",
        "            # Assuming task2's accuracy can be calculated similarly; adjust if not\n",
        "            pred2 = output2.argmax(dim=1, keepdim=True)\n",
        "            correct2 += pred2.eq(target2.view_as(pred2)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Task1 Accuracy: {correct1}/{len(test_loader.dataset)} ({100. * correct1 / len(test_loader.dataset):.0f}%), Task2 Accuracy: {correct2}/{len(test_loader.dataset)} ({100. * correct2 / len(test_loader.dataset):.0f}%)\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fd45d98b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd45d98b",
        "outputId": "40da1a1d-bfef-440e-c0b4-65c3fe1fc7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.data= /content/gdrive/My Drive/Pythoncode/W22/faces_4\n",
            "train_dataset.person_labels= ['an2i', 'at33', 'boland', 'bpm', 'ch4f', 'cheyer', 'choon', 'danieln', 'glickman', 'karyadi', 'kawamura', 'kk49', 'megak', 'mitchell', 'night', 'phoebe', 'saavik', 'steffi', 'sz24', 'tammo']\n",
            "train_dataset.expression_labels= ['angry', 'happy', 'neutral', 'sad']\n",
            "Starting training session 0/3\n",
            "Train Epoch: 1 [0/488 (0%)]\tLoss: 4.377934\n",
            "Train Epoch: 2 [0/488 (0%)]\tLoss: 4.391722\n",
            "Train Epoch: 3 [0/488 (0%)]\tLoss: 4.383186\n",
            "Train Epoch: 4 [0/488 (0%)]\tLoss: 4.384188\n",
            "Train Epoch: 5 [0/488 (0%)]\tLoss: 4.395127\n",
            "Train Epoch: 6 [0/488 (0%)]\tLoss: 4.376195\n",
            "Train Epoch: 7 [0/488 (0%)]\tLoss: 4.392867\n",
            "Train Epoch: 8 [0/488 (0%)]\tLoss: 4.391487\n",
            "Train Epoch: 9 [0/488 (0%)]\tLoss: 4.380594\n",
            "Train Epoch: 10 [0/488 (0%)]\tLoss: 4.380799\n",
            "\n",
            "Test set: Average loss: 0.0968, Task1 Accuracy: 7/136 (5%), Task2 Accuracy: 27/136 (20%)\n",
            "\n",
            "Starting training session 1/3\n",
            "Train Epoch: 1 [0/488 (0%)]\tLoss: 4.404438\n",
            "Train Epoch: 2 [0/488 (0%)]\tLoss: 4.369338\n",
            "Train Epoch: 3 [0/488 (0%)]\tLoss: 4.387496\n",
            "Train Epoch: 4 [0/488 (0%)]\tLoss: 4.384243\n",
            "Train Epoch: 5 [0/488 (0%)]\tLoss: 4.389028\n",
            "Train Epoch: 6 [0/488 (0%)]\tLoss: 4.367991\n",
            "Train Epoch: 7 [0/488 (0%)]\tLoss: 4.385114\n",
            "Train Epoch: 8 [0/488 (0%)]\tLoss: 4.384555\n",
            "Train Epoch: 9 [0/488 (0%)]\tLoss: 4.381144\n",
            "Train Epoch: 10 [0/488 (0%)]\tLoss: 4.396965\n",
            "\n",
            "Test set: Average loss: 0.0968, Task1 Accuracy: 7/136 (5%), Task2 Accuracy: 27/136 (20%)\n",
            "\n",
            "Starting training session 2/3\n",
            "Train Epoch: 1 [0/488 (0%)]\tLoss: 4.368073\n",
            "Train Epoch: 2 [0/488 (0%)]\tLoss: 4.371194\n",
            "Train Epoch: 3 [0/488 (0%)]\tLoss: 4.399909\n",
            "Train Epoch: 4 [0/488 (0%)]\tLoss: 4.380930\n",
            "Train Epoch: 5 [0/488 (0%)]\tLoss: 4.362369\n",
            "Train Epoch: 6 [0/488 (0%)]\tLoss: 4.400690\n",
            "Train Epoch: 7 [0/488 (0%)]\tLoss: 4.382658\n",
            "Train Epoch: 8 [0/488 (0%)]\tLoss: 4.380848\n",
            "Train Epoch: 9 [0/488 (0%)]\tLoss: 4.380181\n",
            "Train Epoch: 10 [0/488 (0%)]\tLoss: 4.374325\n",
            "\n",
            "Test set: Average loss: 0.0968, Task1 Accuracy: 7/136 (5%), Task2 Accuracy: 27/136 (20%)\n",
            "\n",
            "Finished Training\n",
            "Total training time: 74.42 seconds\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "def main(args=None):\n",
        "    if args is None:\n",
        "        # When the script is run in a Jupyter notebook, ignore the command-line arguments\n",
        "        args = sys.argv[1:]\n",
        "        args = [arg for arg in args if not arg.startswith('-f')]\n",
        "\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch Faces MultiTask Classifier Training')\n",
        "    parser.add_argument('--data', type=str, default='/content/gdrive/My Drive/Pythoncode/W22/faces_4', metavar='N',\n",
        "                        help='Path to directory containing faces dataset.')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                        help='number of epochs to train (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                        help='SGD momentum (default: 0.9)')\n",
        "\n",
        "\n",
        "    # Parse only the known arguments and ignore the rest\n",
        "    args, unknown = parser.parse_known_args(args)\n",
        "\n",
        "    print(f\"args.data= {args.data}\")\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Adjust dataset for multitask\n",
        "    train_dataset = CMUFaceDataset(root_dir=os.path.join(args.data, 'train'), transform=transform)\n",
        "    test_dataset = CMUFaceDataset(root_dir=os.path.join(args.data, 'test'), transform=transform)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    # Ensure these numbers are correctly determined based on your dataset\n",
        "    num_classes_taskA = len(train_dataset.person_labels)  # Number of unique persons\n",
        "    num_classes_taskB = len(train_dataset.expression_labels)  # Number of unique expressions\n",
        "\n",
        "    print(f\"train_dataset.person_labels= {train_dataset.person_labels}\\ntrain_dataset.expression_labels= {train_dataset.expression_labels}\")\n",
        "\n",
        "\n",
        "    model = MultiTaskLeNet5(num_classes_taskA=num_classes_taskA, num_classes_taskB=num_classes_taskB).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "\n",
        "# Specify the loss functions for each task\n",
        "    criterion1 = nn.CrossEntropyLoss()  # For the first task, assuming it's classification\n",
        "    criterion2 = nn.CrossEntropyLoss()  # For the first task, assuming it's classification\n",
        "\n",
        "\n",
        "    #show_test_images(train_dataset, train_loader)\n",
        "\n",
        "    model_path = '/content/gdrive/My Drive/Pythoncode/W22/modelMultitask.pth'\n",
        "\n",
        "    # Check if a pre-trained model exists\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    num_training_sessions = 3  # Define the number of training sessions\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for session in range(num_training_sessions):\n",
        "        print(f\"Starting training session {session}/{num_training_sessions}\")\n",
        "\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            train(model, device, train_loader, optimizer, criterion1, criterion2, epoch)\n",
        "\n",
        "\n",
        "        test(model, device, test_loader, criterion1, criterion2)\n",
        "        # Save the model after each epoch\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    print('Total training time: {:.2f} seconds'.format(end_time - start_time))\n",
        "\n",
        "    # Use torchinfo to summarize the model\n",
        "    # You need to specify the input size (including the batch size)\n",
        "    # CIRAR10 dataset consists of 32 x32 color images, the dimensions for each image are: Channels=3, height='32' pixels, width='32' pixels\n",
        "    # Hence my input image is 3 channel, 32x32 pixels, and you're using a batch size of 32:\n",
        "    input_size = (64, 1, 32, 32)  # Format: (batch_size, channels-colour, RGB, height, width)\n",
        "    summary(model, input_size)\n",
        "\n",
        "    #show_test_images(test_dataset, test_loader)\n",
        "    #test(model, device, test_loader, criterion)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ObM8zWZAug-_",
      "metadata": {
        "id": "ObM8zWZAug-_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZyUHc6sxuhMd",
      "metadata": {
        "id": "ZyUHc6sxuhMd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b19120",
      "metadata": {
        "id": "79b19120"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}