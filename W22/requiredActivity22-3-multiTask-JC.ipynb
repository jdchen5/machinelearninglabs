{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdchen5/machinelearninglabs/blob/main/W22/requiredActivity22-3-multiTask-JC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "AyXFDj5z5OsF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyXFDj5z5OsF",
        "outputId": "40138e2b-5f31-4b6b-ce79-c20e7ed987e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEgNzGSD7pps",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEgNzGSD7pps",
        "outputId": "5b2131ef-0de5-459e-e740-f711d6247167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "S55oDpGBLlv7",
        "outputId": "7b157476-f8e2-406f-a171-6e4da669a24e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "S55oDpGBLlv7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only required at the beinning to split the traing and set data"
      ],
      "metadata": {
        "id": "nRb6cqktKif-"
      },
      "id": "nRb6cqktKif-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_miGbcPG7uks",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_miGbcPG7uks",
        "outputId": "2197ce6f-9fe9-4f2f-9783-36f48106f305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['choon', 'ch4f', 'karyadi', 'bpm', 'an2i', 'glickman', 'at33', 'danieln', 'boland', 'cheyer', 'night', 'saavik', 'mitchell', 'kk49', 'steffi', 'kawamura', 'megak', 'sz24', 'phoebe', 'tammo']\n",
            "Data split into train and test directories.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to the dataset\n",
        "data_dir = '/content/gdrive/My Drive/Pythoncode/W22/faces_4'\n",
        "# Get all class directories\n",
        "class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "print(class_dirs)\n",
        "\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Split each class's files and move them to the corresponding train/test directories\n",
        "for class_dir in class_dirs:\n",
        "    # Full path to the class directory\n",
        "    class_path = os.path.join(data_dir, class_dir)\n",
        "    # Get all files in the class directory\n",
        "    files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "    # Check if there are any files to split\n",
        "    if not files:\n",
        "        print(f\"No files to split in directory {class_dir}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Split the files into 80% train and 20% test\n",
        "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create corresponding class directories in train and test directories\n",
        "    train_class_dir = os.path.join(train_dir, class_dir)\n",
        "    test_class_dir = os.path.join(test_dir, class_dir)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Function to copy files to the specified directory\n",
        "    def copy_files(files, source_dir, destination_dir):\n",
        "        for file in files:\n",
        "            shutil.copy(os.path.join(source_dir, file), os.path.join(destination_dir, file))\n",
        "\n",
        "    # Copy the files to their respective directories\n",
        "    copy_files(train_files, class_path, train_class_dir)\n",
        "    copy_files(test_files, class_path, test_class_dir)\n",
        "\n",
        "print(\"Data split into train and test directories.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c7g5BYQJWYAl",
      "metadata": {
        "id": "c7g5BYQJWYAl"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "26d377b5",
      "metadata": {
        "id": "26d377b5"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3419b75b",
      "metadata": {
        "id": "3419b75b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img = cv2.imread(\"some_image.pgm\", cv2.IMREAD_COLOR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "FMSlLxBruSTp",
      "metadata": {
        "id": "FMSlLxBruSTp"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e74202bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74202bf",
        "outputId": "31fca7c2-2613-4b05-a562-09f92e79d8a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an2i',\n",
              " 'at33',\n",
              " 'boland',\n",
              " 'bpm',\n",
              " 'ch4f',\n",
              " 'cheyer',\n",
              " 'choon',\n",
              " 'danieln',\n",
              " 'glickman',\n",
              " 'karyadi',\n",
              " 'kawamura',\n",
              " 'kk49',\n",
              " 'megak',\n",
              " 'mitchell',\n",
              " 'night',\n",
              " 'phoebe',\n",
              " 'saavik',\n",
              " 'steffi',\n",
              " 'sz24',\n",
              " 'tammo',\n",
              " 'test',\n",
              " 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Custom helper function to load images in PGM format using OpenCV\n",
        "def img_loader(path):\n",
        "    return cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "data = ImageFolder(root='/content/gdrive/My Drive/Pythoncode/W22/faces_4/', loader=img_loader, transform=transforms)\n",
        "data.classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#custome class for face and expression\n",
        "class CMUFaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.person_labels = set()\n",
        "        self.expression_labels = set()\n",
        "\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pgm'):\n",
        "                    img_path = os.path.join(subdir, file)\n",
        "                    self.images.append(img_path)\n",
        "\n",
        "                    # Extract and store person and expression labels\n",
        "                    person_label, expression_label = self._extract_labels(img_path)\n",
        "                    self.person_labels.add(person_label)\n",
        "                    self.expression_labels.add(expression_label)\n",
        "\n",
        "        # Convert sets to sorted lists for consistent indexing\n",
        "        self.person_labels = sorted(list(self.person_labels))\n",
        "        self.expression_labels = sorted(list(self.expression_labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        person_label, expression_label = self._extract_labels(img_path)\n",
        "        person_idx = self.person_labels.index(person_label)\n",
        "        expression_idx = self.expression_labels.index(expression_label)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, (person_idx, expression_idx)\n",
        "\n",
        "    def _extract_labels(self, img_path):\n",
        "        \"\"\"\n",
        "        Extracts person and expression labels from an image file path.\n",
        "        \"\"\"\n",
        "        basename = os.path.basename(img_path)\n",
        "        parts = basename.split('_')\n",
        "        person_label = parts[0]\n",
        "        expression_label = parts[2]  # Adjust index based on your file naming convention\n",
        "        return person_label, expression_label\n"
      ],
      "metadata": {
        "id": "-Cpj_AECLJBG"
      },
      "id": "-Cpj_AECLJBG",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "xp83mM171IiJ",
      "metadata": {
        "id": "xp83mM171IiJ"
      },
      "outputs": [],
      "source": [
        "# Get some random dataset  images\n",
        "\n",
        "def show_test_images(transform, test_dataset, test_loader):\n",
        "    # Get some random test images\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Show images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "    # Get the class names from the 'test_dataset'\n",
        "    class_names = test_dataset.classes\n",
        "\n",
        "    # Print labels with class names\n",
        "    print('GroundTruth: ', ' '.join('%5s' % class_names[labels[j]] for j in labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47cfee25",
      "metadata": {
        "id": "47cfee25"
      },
      "outputs": [],
      "source": [
        "# Define a CNN architecture inspired by LeNet5, adjusted for multi-tasks\n",
        "class MultiTaskLeNet5(nn.Module):\n",
        "    def __init__(self, num_classes_taskA, num_classes_taskB):\n",
        "        super(MultiTaskLeNet5, self).__init__()\n",
        "        # Shared layers\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # Placeholder for the number of flat features, to be initialized later\n",
        "        self.num_flat_features = None\n",
        "\n",
        "        # Task A specific layers\n",
        "        # We use placeholders for the input dimension, which we will set later\n",
        "        self.fc1_taskA = nn.Linear(None, 120)  # Placeholder, actual value set in forward or another method\n",
        "        self.fc2_taskA = nn.Linear(120, 84)\n",
        "        self.fc3_taskA = nn.Linear(84, num_classes_taskA)\n",
        "\n",
        "        # Task B specific layers\n",
        "        # We use placeholders for the input dimension, which we will set later\n",
        "        self.fc1_taskB = nn.Linear(None, 120)  # Placeholder, actual value set in forward or another method\n",
        "        self.fc2_taskB = nn.Linear(120, 50)\n",
        "        self.fc3_taskB = nn.Linear(50, num_classes_taskB)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #print(f\"Input batch size: {x.size(0)}\")  # Should always be 64 based on your DataLoader\n",
        "\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        #print(x.size())  # Add this line to check the size of the output here\n",
        "\n",
        "       # Dynamically calculate the number of flat features\n",
        "        if self.num_flat_features is None:\n",
        "            self.num_flat_features = x.view(x.size(0), -1).size(1)\n",
        "            # Now that we have the number of flat features, initialize the fc layers properly\n",
        "            self.fc1_taskA = nn.Linear(self.num_flat_features, 120).to(x.device)\n",
        "            self.fc1_taskB = nn.Linear(self.num_flat_features, 120).to(x.device)\n",
        "\n",
        "        # Flatten the features for the fully connected layers\n",
        "        x = x.view(-1, self.num_flat_features)\n",
        "\n",
        "       # Task A path\n",
        "        x_a = F.relu(self.fc1_taskA(x))\n",
        "        x_a = F.relu(self.fc2_taskA(x_a))\n",
        "        x_a = self.fc3_taskA(x_a)\n",
        "\n",
        "        # Task B path\n",
        "        x_b = F.relu(self.fc1_taskB(x))\n",
        "        x_b = F.relu(self.fc2_taskB(x_b))\n",
        "        x_b = self.fc3_taskB(x_b)\n",
        "\n",
        "        #print(f\"Input batch size: {x.size(0)}\")  # Should be 64\n",
        "        #print(f\"Output batch size: {x.size(0)}\")  # Should also be 64\n",
        "\n",
        "        return x_a, x_b\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PSoJAGL8Zy_x",
      "metadata": {
        "id": "PSoJAGL8Zy_x"
      },
      "outputs": [],
      "source": [
        "# Define a function to reset the model\n",
        "def reset_model(num_classes, device):\n",
        "    model = Net(num_classes=num_classes).to(device)\n",
        "    return model\n",
        "\n",
        "# Define a function to reset the optimizer\n",
        "def reset_optimizer(model, lr=0.01, momentum=0.9):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "RMXn4wl25z8k",
      "metadata": {
        "id": "RMXn4wl25z8k"
      },
      "outputs": [],
      "source": [
        "# Train and Test function\n",
        "def train(model, device, train_loader, optimizer, criterion1, criterion2, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, (targetsA, targetsB)) in enumerate(train_loader):\n",
        "        data, targetsA, targetsB = data.to(device), targetsA.to(device), targetsB.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputsA, outputsB = model(data)\n",
        "        lossA = criterion1(outputsA, targetsA)\n",
        "        lossB = criterion2(outputsB, targetsB)\n",
        "        loss = lossA + lossB  # Combine losses; adjust if you're weighting tasks differently\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:  # Adjust log interval as needed\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test(model, device, test_loader, criterion1, criterion2):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target1, target2 = data.to(device), target[0].to(device), target[1].to(device)\n",
        "            output1, output2 = model(data)\n",
        "            test_loss += (criterion1(output1, target1) + criterion2(output2, target2)).item()  # Sum up batch loss\n",
        "            pred1 = output1.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct1 += pred1.eq(target1.view_as(pred1)).sum().item()\n",
        "            # Assuming task2's accuracy can be calculated similarly; adjust if not\n",
        "            pred2 = output2.argmax(dim=1, keepdim=True)\n",
        "            correct2 += pred2.eq(target2.view_as(pred2)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Task1 Accuracy: {correct1}/{len(test_loader.dataset)} ({100. * correct1 / len(test_loader.dataset):.0f}%), Task2 Accuracy: {correct2}/{len(test_loader.dataset)} ({100. * correct2 / len(test_loader.dataset):.0f}%)\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fd45d98b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "fd45d98b",
        "outputId": "01071d52-5e71-4135-9965-30564de989d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.data= /content/gdrive/My Drive/Pythoncode/W22/faces_4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-39dd03eaf1fe>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-39dd03eaf1fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mnum_classes_taskB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpression_labels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Number of unique expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTaskLeNet5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes_taskA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes_taskA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_taskB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes_taskB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b0b45398de12>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes_taskA, num_classes_taskB)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Task A specific layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# We use placeholders for the input dimension, which we will set later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_taskA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Placeholder, actual value set in forward or another method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_taskA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3_taskA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_taskA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\""
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "def main(args=None):\n",
        "    if args is None:\n",
        "        # When the script is run in a Jupyter notebook, ignore the command-line arguments\n",
        "        args = sys.argv[1:]\n",
        "        args = [arg for arg in args if not arg.startswith('-f')]\n",
        "\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch Faces MultiTask Classifier Training')\n",
        "    parser.add_argument('--data', type=str, default='/content/gdrive/My Drive/Pythoncode/W22/faces_4', metavar='N',\n",
        "                        help='Path to directory containing faces dataset.')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                        help='number of epochs to train (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                        help='SGD momentum (default: 0.9)')\n",
        "\n",
        "\n",
        "    # Parse only the known arguments and ignore the rest\n",
        "    args, unknown = parser.parse_known_args(args)\n",
        "\n",
        "    print(f\"args.data= {args.data}\")\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Adjust dataset for multitask\n",
        "    train_dataset = CMUFaceDataset(root_dir=os.path.join(args.data, 'train'), transform=transform)\n",
        "    test_dataset = CMUFaceDataset(root_dir=os.path.join(args.data, 'test'), transform=transform)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    # Ensure these numbers are correctly determined based on your dataset\n",
        "    num_classes_taskA = len(train_dataset.person_labels)  # Number of unique persons\n",
        "    num_classes_taskB = len(train_dataset.expression_labels)  # Number of unique expressions\n",
        "\n",
        "    model = MultiTaskLeNet5(num_classes_taskA=num_classes_taskA, num_classes_taskB=num_classes_taskB).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "\n",
        "# Specify the loss functions for each task\n",
        "    criterion1 = nn.CrossEntropyLoss()  # For the first task, assuming it's classification\n",
        "    criterion2 = nn.CrossEntropyLoss()  # For the first task, assuming it's classification\n",
        "\n",
        "\n",
        "    show_test_images(transform, train_dataset, train_loader)\n",
        "\n",
        "    model_path = '/content/gdrive/My Drive/Pythoncode/W22/modelMultitask.pth'\n",
        "\n",
        "    # Check if a pre-trained model exists\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    num_training_sessions = 3  # Define the number of training sessions\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for session in range(num_training_sessions):\n",
        "        print(f\"Starting training session {session}/{num_training_sessions}\")\n",
        "\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            train(model, device, train_loader, optimizer, criterion1, criterion2, epoch)\n",
        "\n",
        "\n",
        "        #test(model, device, test_loader, criterion1, criterion2)\n",
        "        # Save the model after each epoch\n",
        "        #torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    print('Total training time: {:.2f} seconds'.format(end_time - start_time))\n",
        "\n",
        "    #show_test_images(transform, test_dataset, test_loader)\n",
        "    #test(model, device, test_loader, criterion)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ObM8zWZAug-_",
      "metadata": {
        "id": "ObM8zWZAug-_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZyUHc6sxuhMd",
      "metadata": {
        "id": "ZyUHc6sxuhMd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b19120",
      "metadata": {
        "id": "79b19120"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}