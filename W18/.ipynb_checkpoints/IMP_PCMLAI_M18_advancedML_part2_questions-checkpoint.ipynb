{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a9caaf",
   "metadata": {
    "id": "f5a9caaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924283c",
   "metadata": {
    "id": "e924283c"
   },
   "source": [
    "### 1. Optimisation Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f951e",
   "metadata": {
    "id": "711f951e"
   },
   "source": [
    "Let us have a quick introduction to optimization. Assume $f: \\mathbb{R}^n \\mapsto \\mathbb{R}$ is a convex function. A convex optimization problem is \n",
    "\n",
    "\\begin{align}\\tag{convex opt}\n",
    "\\mathrm{minimize} f(x) \\quad \\mathrm{subject\\;to} \\  x \\in \\mathcal{F} \\subset \\mathbb{R^n}.\n",
    "\\end{align}\n",
    "\n",
    "A solution $x^\\star$ is called the *global minimizer* if for every $x \\in \\mathcal{F}$ we have\n",
    "$$ f(x^\\star) \\leq f(x). $$\n",
    "\n",
    "There are well-known results that say such a global minimizer exists when $\\mathcal{F}$ is a closed and bounded set. Closed means, roughly, this set includes its limit points (*e.g.*, $(0,1)$ is open but $[0,1]$ is closed), and bounded means this set is not something like $[0, \\infty)$ (intuitively, pick a direction in the set, if this set is bounded then you cannot go in this direction forever without leaving the set).\n",
    "\n",
    "Now assume $\\mathcal{F} = \\mathbb{R}^n$, which means we are solving an *unconstrained optimization problem*. Calculus tells us that, if $f$ is convex, and if the gradient of $f$ (which is denoted by $\\nabla f$) exists and is continuous, then a point $x \\in \\mathbb{R}^n$ is the global minimizer if and only if $\\nabla f(x) = 0$. However, in general, finding a point that satisfy this *first order condition* ($\\nabla f(x) = 0$) is not immediate. To this end, there are several algorithms proposed to iteratively update a candidate solution until this optimality condition is met,.\n",
    "\n",
    "The most used algorithm is named the *gradient descent method*. The algorithm first fixes the iteration number $k=0$ and a starting point $x_0 \\in \\mathbb{R}^n$. Then, the next candidate solution $x_1$ is constructed as $x_{1} = x_0 - \\alpha_0 \\cdot\\nabla f(x_0) $. Here, $\\alpha_0 > 0$ is a constant named the *step size*. We can see that $x_1$ is constructed by taking the previous iteration's solution, $x_0$, and going in the $- \\nabla f(x_0)$ direction by a step size of $\\alpha_0$. The algorithm keeps iterating for $k= 1,2,\\ldots$ by the same rule:\n",
    "$$x_{k+1} = x_{k}-  \\alpha_k \\cdot \\nabla f(x_{k-1}), \\tag{gradient descent}$$\n",
    "and stops when $\\nabla{f}(x_k) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7930b7",
   "metadata": {
    "id": "3c7930b7"
   },
   "source": [
    "**Exercise:**\n",
    "Solve the following problem analytically.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{minimize} \\ f(x) = (x_1 - 2)^2 + (3 \\cdot x_2 - 4)^2 \\quad \\mathrm{subject\\;to} \\ x \\in \\mathbb{R}^2.\n",
    "\\end{align}\n",
    "\n",
    "Then, solve via gradient descent and report the number of iteration it takes for the algorithm to converge. For the algorithm take the starting point $x_0 = (0, \\ 0)$ and fix $\\alpha_k = 0.01$ for all iterations. Moreover, for a stopping condition, take when the 2-norm of the gradient, $||\\nabla f(x_k)||_2$, is upper bounded by $10^{-6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee29ea",
   "metadata": {
    "id": "5fee29ea"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Analytic: we can derive that $\\nabla f(x) = ... $ and when we set this to 0 we will have $x^\\star = ...$.\n",
    "\n",
    "The gradient descent algorithm is implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea1759d",
   "metadata": {
    "id": "cea1759d"
   },
   "outputs": [],
   "source": [
    "def fn(x1, x2):\n",
    "    return (x1 - 2)**2 + (3*x2 - 4)**2\n",
    "def grad(x1, x2):\n",
    "    return np.array([2*x1 - 4, 18*x2 - 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276ebffd",
   "metadata": {
    "id": "276ebffd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754, array([1.99999951, 1.33333333]), 2.446414171549208e-13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent parameters\n",
    "alpha = 0.01  # Step size\n",
    "tolerance = 1e-6\n",
    "max_iterations = 10000  # Just in case it doesn't converge, to avoid infinite loop\n",
    "\n",
    "# Starting point\n",
    "x_k = np.array([0.0, 0.0])\n",
    "\n",
    "# Gradient descent iteration\n",
    "for k in range(max_iterations):\n",
    "    gradient = grad(x_k[0], x_k[1])\n",
    "    if np.linalg.norm(gradient) < tolerance:\n",
    "        break\n",
    "    x_k = x_k - alpha * gradient\n",
    "\n",
    "# The number of iterations and the minimum point found\n",
    "num_iterations = k + 1\n",
    "x_min = x_k\n",
    "f_min = fn(x_min[0],x_min[1])\n",
    "\n",
    "num_iterations, x_min, f_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52a7051",
   "metadata": {
    "id": "a52a7051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal objective value of 2.446414171549208e-13 with the solution [1.99999951 1.33333333] in 754 iterations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal objective value of\", f_min,\\\n",
    "      \"with the solution\", x_min, \"in\",  num_iterations, \"iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029d9db",
   "metadata": {
    "id": "4029d9db"
   },
   "source": [
    "### 2. Optimization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1900a1e",
   "metadata": {
    "id": "f1900a1e"
   },
   "source": [
    "Recall that in neural networks the optimization variables are the weights of the network. You may ask:\n",
    "1. What is the optimization function we are interested in? Is it convex?\n",
    "2. How can we use gradient descent for neural networks?\n",
    "3. How do we compute the gradients in a complicated network?\n",
    "4. How do we initialize weights in a neural network?\n",
    "\n",
    "It turns out that, although the standard loss functions are convex in their inputs, they are not convex in the optimization variables in the concept of neural networks. This is due to the compositions we apply in neural networks (recall the previous notebook -- namely, the optimization variables are transformed by several compositions before evaluating the decision). \n",
    "\n",
    "For neural networks, using gradient descent is perfectly fine, however, we will see a variant of it named *stochastic gradient descent method* which will improve the speed and performance of the algorithm.\n",
    "\n",
    "Finally, to compute the gradient of the loss function with respect to the weights, we will learn a concept named *backpropagation*. We will concentrate on these topics more now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd5df1",
   "metadata": {
    "id": "6cdd5df1"
   },
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b8071",
   "metadata": {
    "id": "d76b8071"
   },
   "source": [
    "The loss functions used in neural networks are the same with the ones we used in previous weeks. For example, if we are interested in regression with neural networks, then we may be interested in the loss $L = ||y - \\hat{y}||^2_2$ where $y$ is the vector of the true target values, and $\\hat{y}$ is our estimation that we learned from the predictors. So, if we have $i = 1,\\ldots, n$ training instances, and let $L_i := (y_i - \\hat{y}_i)^2$, then our loss function can be written as:\n",
    "$$ L := L_1 + L_2 + \\ldots + L_n = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\ldots +(y_n - \\hat{y}_n)^2.$$\n",
    "This function is convex in $\\hat{y_i}$ for all $i = 1,\\ldots,n$, but **this does not mean the optimization problem is convex**. The issue is that, in neural networks we cannot directly optimize $\\hat{y_i}$. We need to learn a function that uses the predictors of the input and gives us an estimation $\\hat{y_i}$ by optimizing some network weights, and the loss function is typically **not** convex in these weights. Let us work on an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4edfacc",
   "metadata": {
    "id": "b4edfacc"
   },
   "source": [
    "**Question**\n",
    "Find the prediction $\\hat{y}$ for the input $x= ( x_1 = 2, \\ x_2 = -3)$ of the following neural network with a single hidden layer.\n",
    "<img src=\"forward.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7939f",
   "metadata": {
    "id": "69d7939f"
   },
   "source": [
    "**Answer**\n",
    "We first compute the outputs of the neurons $r_1$ and $r_2$ on the hidden layer, and then proceed to the output $s$.\n",
    "\n",
    "- The input to $r_1$ is\n",
    "$(-1,3,-0.1)\\cdot (1,2,-3)\n",
    "= -1\\cdot 1+3\\cdot 2+(-0.1)\\cdot(-3)\n",
    "= 5.3$,\n",
    "so its output is $\\max(0,5.3)=0$.\n",
    "- The input to $r_2$ is\n",
    "$(0.2,-1,0.5)\\cdot(1,2,-3) = -3.3$,\n",
    "so its output is $\\max(0,-3.3)=0$.\n",
    "- The input to $s$ is\n",
    "$(-0.2,0.4)\\cdot(5.3,0) = -1.06$.\n",
    "Its output, applying the sigmoid function $e^{-1.06}/(1+e^{-1.06})$, is $0.2573$.\n",
    "\n",
    "If this is a binary classification setting this means that the neural network returns probability\n",
    "$0.2573$ for class 1 and $0.7427$ for class 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64c5b4",
   "metadata": {
    "id": "8f64c5b4"
   },
   "source": [
    "**Question** Use `torch` to answer the question above by using the scripts we derived in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36471f7c",
   "metadata": {
    "id": "36471f7c"
   },
   "source": [
    "**Answer** Omitted for space purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b839f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the first layer: tensor([ 5.3000, -3.3000], grad_fn=<AddBackward0>)\n",
      "Hidden layer output (previous_layer_output): tensor([5.3000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "Final output (probability): 0.2573094367980957\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"359pt\" height=\"760pt\"\n",
       " viewBox=\"0.00 0.00 359.00 760.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 756)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-756 355,-756 355,4 -4,4\"/>\n",
       "<!-- 1349614043760 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1349614043760</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"266,-32 212,-32 212,0 266,0 266,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"239\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1349541531408 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1349541531408</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"295,-88 183,-88 183,-68 295,-68 295,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"239\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541531408&#45;&gt;1349614043760 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>1349541531408&#45;&gt;1349614043760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239,-67.62C239,-61.1 239,-52.05 239,-43.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"242.5,-43.65 239,-33.65 235.5,-43.65 242.5,-43.65\"/>\n",
       "</g>\n",
       "<!-- 1349541533520 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1349541533520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"283,-144 195,-144 195,-124 283,-124 283,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"239\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541533520&#45;&gt;1349541531408 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1349541533520&#45;&gt;1349541531408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239,-123.59C239,-117.01 239,-107.96 239,-99.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"242.5,-99.81 239,-89.81 235.5,-99.81 242.5,-99.81\"/>\n",
       "</g>\n",
       "<!-- 1349541533328 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1349541533328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"233,-200 121,-200 121,-180 233,-180 233,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 1349541533328&#45;&gt;1349541533520 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1349541533328&#45;&gt;1349541533520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.8,-179.59C196.6,-171.93 209.25,-160.91 219.77,-151.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"221.94,-154.5 227.18,-145.29 217.34,-149.22 221.94,-154.5\"/>\n",
       "</g>\n",
       "<!-- 1349541538560 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1349541538560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-262 136,-262 136,-242 218,-242 218,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-248.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541538560&#45;&gt;1349541533328 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1349541538560&#45;&gt;1349541533328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177,-241.62C177,-233.56 177,-221.65 177,-211.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.5,-211.63 177,-201.63 173.5,-211.63 180.5,-211.63\"/>\n",
       "</g>\n",
       "<!-- 1349541541296 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1349541541296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"180,-324 56,-324 56,-304 180,-304 180,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541541296&#45;&gt;1349541538560 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1349541541296&#45;&gt;1349541538560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.22,-303.62C136.02,-294.68 149.48,-280.99 160.18,-270.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.46,-272.78 166.98,-263.19 157.47,-267.87 162.46,-272.78\"/>\n",
       "</g>\n",
       "<!-- 1349541540480 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1349541540480</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"165,-380 71,-380 71,-360 165,-360 165,-380\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-366.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541540480&#45;&gt;1349541541296 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1349541540480&#45;&gt;1349541541296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-359.59C118,-353.01 118,-343.96 118,-335.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.5,-335.81 118,-325.81 114.5,-335.81 121.5,-335.81\"/>\n",
       "</g>\n",
       "<!-- 1349541541632 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1349541541632</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"162,-442 74,-442 74,-422 162,-422 162,-442\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-428.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541541632&#45;&gt;1349541540480 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1349541541632&#45;&gt;1349541540480</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-421.62C118,-413.56 118,-401.65 118,-391.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.5,-391.63 118,-381.63 114.5,-391.63 121.5,-391.63\"/>\n",
       "</g>\n",
       "<!-- 1349541541776 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1349541541776</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"112,-504 0,-504 0,-484 112,-484 112,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 1349541541776&#45;&gt;1349541541632 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1349541541776&#45;&gt;1349541541632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.69,-483.62C74.93,-474.68 89.08,-460.99 100.33,-450.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"102.74,-452.63 107.5,-443.17 97.88,-447.6 102.74,-452.63\"/>\n",
       "</g>\n",
       "<!-- 1349541541488 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1349541541488</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"97,-566 15,-566 15,-546 97,-546 97,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541541488&#45;&gt;1349541541776 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1349541541488&#45;&gt;1349541541776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56,-545.62C56,-537.56 56,-525.65 56,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.5,-515.63 56,-505.63 52.5,-515.63 59.5,-515.63\"/>\n",
       "</g>\n",
       "<!-- 1349613922976 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>1349613922976</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-628 18,-628 18,-608 94,-608 94,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1349613922976&#45;&gt;1349541541488 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1349613922976&#45;&gt;1349541541488</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56,-607.62C56,-599.56 56,-587.65 56,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.5,-577.63 56,-567.63 52.5,-577.63 59.5,-577.63\"/>\n",
       "</g>\n",
       "<!-- 1349613920432 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>1349613920432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"106,-684 6,-684 6,-664 106,-664 106,-684\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1349613920432&#45;&gt;1349613922976 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1349613920432&#45;&gt;1349613922976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56,-663.59C56,-657.01 56,-647.96 56,-639.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.5,-639.81 56,-629.81 52.5,-639.81 59.5,-639.81\"/>\n",
       "</g>\n",
       "<!-- 1349541447936 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>1349541447936</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"91,-752 21,-752 21,-720 91,-720 91,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">W1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-726.5\" font-family=\"monospace\" font-size=\"10.00\"> (2, 3)</text>\n",
       "</g>\n",
       "<!-- 1349541447936&#45;&gt;1349613920432 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1349541447936&#45;&gt;1349613920432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56,-719.55C56,-712.34 56,-703.66 56,-695.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.5,-695.92 56,-685.92 52.5,-695.92 59.5,-695.92\"/>\n",
       "</g>\n",
       "<!-- 1349541541584 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1349541541584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"230,-504 130,-504 130,-484 230,-484 230,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1349541541584&#45;&gt;1349541541632 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>1349541541584&#45;&gt;1349541541632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.31,-483.62C161.07,-474.68 146.92,-460.99 135.67,-450.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.12,-447.6 128.5,-443.17 133.26,-452.63 138.12,-447.6\"/>\n",
       "</g>\n",
       "<!-- 1349613842608 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1349613842608</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"209,-572 151,-572 151,-540 209,-540 209,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">W1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 1349613842608&#45;&gt;1349541541584 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1349613842608&#45;&gt;1349541541584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180,-539.55C180,-532.34 180,-523.66 180,-515.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.5,-515.92 180,-505.92 176.5,-515.92 183.5,-515.92\"/>\n",
       "</g>\n",
       "<!-- 1349541541344 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>1349541541344</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"274,-324 198,-324 198,-304 274,-304 274,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1349541541344&#45;&gt;1349541538560 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1349541541344&#45;&gt;1349541538560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M226.78,-303.62C217.98,-294.68 204.52,-280.99 193.82,-270.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.53,-267.87 187.02,-263.19 191.54,-272.78 196.53,-267.87\"/>\n",
       "</g>\n",
       "<!-- 1349541541152 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>1349541541152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"286,-380 186,-380 186,-360 286,-360 286,-380\"/>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-366.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1349541541152&#45;&gt;1349541541344 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>1349541541152&#45;&gt;1349541541344</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236,-359.59C236,-353.01 236,-343.96 236,-335.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.5,-335.81 236,-325.81 232.5,-335.81 239.5,-335.81\"/>\n",
       "</g>\n",
       "<!-- 1349541458416 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>1349541458416</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"271,-448 201,-448 201,-416 271,-416 271,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">W2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-422.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 2)</text>\n",
       "</g>\n",
       "<!-- 1349541458416&#45;&gt;1349541541152 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>1349541458416&#45;&gt;1349541541152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236,-415.55C236,-408.34 236,-399.66 236,-391.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.5,-391.92 236,-381.92 232.5,-391.92 239.5,-391.92\"/>\n",
       "</g>\n",
       "<!-- 1349541533472 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>1349541533472</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"351,-200 251,-200 251,-180 351,-180 351,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1349541533472&#45;&gt;1349541533520 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>1349541533472&#45;&gt;1349541533520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.2,-179.59C281.4,-171.93 268.75,-160.91 258.23,-151.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.66,-149.22 250.82,-145.29 256.06,-154.5 260.66,-149.22\"/>\n",
       "</g>\n",
       "<!-- 1349614044480 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>1349614044480</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"330,-268 272,-268 272,-236 330,-236 330,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">W2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-242.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1349614044480&#45;&gt;1349541533472 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>1349614044480&#45;&gt;1349541533472</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M301,-235.55C301,-228.34 301,-219.66 301,-211.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.5,-211.92 301,-201.92 297.5,-211.92 304.5,-211.92\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x13a36f19240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as func \n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot \n",
    "\n",
    "\n",
    "# Adjust the model to match the neural network from the image\n",
    "model = nn.Sequential()\n",
    "model.add_module('W1', nn.Linear(3, 2))  # First linear layer\n",
    "model.add_module('relu', nn.ReLU())      # ReLU activation\n",
    "model.add_module('W2', nn.Linear(2, 1))  # Second linear layer\n",
    "#model.add_module('logit', nn.Sigmoid())  # Sigmoid activation\n",
    "\n",
    "# Define the weights for the model\n",
    "weights_W1 = torch.tensor([[-1, 3, -0.1], [0.2, -1, 0.5]])  # Weights for W1\n",
    "weights_W2 = torch.tensor([[-0.2, 0.4]])  # Weights for W2\n",
    "\n",
    "# Set the weights for the model (assuming biases are zero for this example)\n",
    "model[0].weight.data = weights_W1\n",
    "model[0].bias.data = torch.zeros(2)  # Assuming zero biases for W1\n",
    "model[2].weight.data = weights_W2\n",
    "model[2].bias.data = torch.zeros(1)  # Assuming zero biases for W2\n",
    "\n",
    "# Define the input tensor with 3 features as per the image\n",
    "x = torch.tensor([1, 2, -3], dtype=torch.float32)\n",
    "\n",
    "# Perform a forward pass through the first part of the network\n",
    "hidden_output = model[0](x)  # Output of the first layer\n",
    "# Print the output of the first layer\n",
    "print(f\"Output of the first layer: {hidden_output}\")\n",
    "hidden_output = model[1](hidden_output)  # Output after ReLU activation\n",
    "\n",
    "# Print the output of the hidden layer\n",
    "print(f\"Hidden layer output (previous_layer_output): {hidden_output}\")\n",
    "\n",
    "# Now apply the second linear layer and the sigmoid activation separately\n",
    "final_linear_output = model[2](hidden_output)\n",
    "final_output = torch.sigmoid(final_linear_output)  # Apply sigmoid activation\n",
    "\n",
    "# Print the final output (probability)\n",
    "print(f\"Final output (probability): {final_output.item()}\")\n",
    "\n",
    "# Normally, here you would visualize the model, but torchviz is not available\n",
    "make_dot(final_output, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898beb1",
   "metadata": {
    "id": "9898beb1"
   },
   "source": [
    "#### Convexity\n",
    "You may have realized that, in the above example, the value we returned, $\\hat{y}$ is the result of a complicated function $\\hat{f}(x;W)$, where $W$ is the collection of weights, and $x$ is the predictors of the input. In the above example, our predition function looks like the following:\n",
    "$$ \\hat{f}(x;W) = s\\left[ -0.2 \\cdot r( 3 \\cdot x_1 - 0.1 \\cdot x_2 - 1)  + 0.4 \\cdot r(-x_1 + 0.5\\cdot x_2 + 0.2) \\right]$$\n",
    "where $s[z]:= e^z / (1 + e^z)$ is the sigmoid function and $r(z) = \\max\\{0,z\\}$ is the ReLU function. If we put these functions in the definition explicitly, then the function will look even more complicated. And, remember that this is just a very small setting with a single hidden layer and two-dimensional input, where in reality we have many hidden layers, many nodes, several activation functions, and high-dimensional inputs. This function is **not** convex in the \"weights\", so if we change the weights (that are written in grey color) of this network (*e.g.*, $-3, -1, -0.1, 0.5, \\ldots$), and keep them as optimization variables, then the loss function $||\\hat{f}(x;W) -  y||$ will not be convex in the elements of $W$ anymore. This will leadthe gradient descent to give a result that is not *globally optimal*, rather we would hope to have a \"good enough\" solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c361d",
   "metadata": {
    "id": "3a5c361d"
   },
   "source": [
    "#### Gradient Descent\n",
    "Although we discussed the gradient descent method will not give the globally optimum solution, we are still interested in finding a good set of weights to the above network. In general, if we keep the weights as variables, we can represent the network as the following:\n",
    "<img src=\"forward_weights.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "Now our goal is to optimize the weights $w_{ji}$ by using a collection of training instances, $(x_1, x_2, y) \\in X_{Tr}$.\n",
    "\n",
    "We will need two things:\n",
    "1. *Initialization of the weights:* It is a common practice to randomly assign weights. As the randomness may result in a poor solution, in general we would be interested in trying several starting weights and starting an optimization procedure in each setting.\n",
    "2. *Computing of the gradients:* How do we, for example, compute $\\dfrac{\\partial (y - \\hat{f}(x;W))^2}{\\partial{w_{21}}}$ where $x = (x_1, \\ x_2)$ and $y$ give a single training instance? For this, we will use the chain rule from calculus, that roughly states:\n",
    "$$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial g}{\\partial h} \\cdot \\dfrac{\\partial h}{\\partial x}.$$\n",
    "Using this iteratively is called the \"backpropagation\" step to compute the gradients.\n",
    "\n",
    "Furthermore, as we minimize the loss over a training set rather than for a single point, our loss will look like $$ \\sum_{(x,y) \\in X_{Tr}}(y - \\hat{f}(x;W))^2 $$\n",
    "and since it will be costly to consider every instance $(x,y)\\in X_{Tr}$, in each step of gradient descent, we will instead consider a random selection of them. This algorithm is named *stochastic gradient descent* and is the rule-of-thumb in optimization for neural networks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
