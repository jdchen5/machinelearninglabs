{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbb1049",
   "metadata": {
    "id": "2cbb1049"
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "from aima3 import learning\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as func \n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2434d2",
   "metadata": {
    "id": "cb2434d2"
   },
   "source": [
    "### Backpropagation and the Chain Rule\n",
    "\n",
    "\n",
    "*Foreword: In this notebook we use slightly different terminology. An arbitrary training instance is denoted as $(v, y) \\in E$ where $v$ is the collection of predictors, $y$ is the target, and $E$ is the training set. Moreover, the network weights are denoted by $x$.* \n",
    "\n",
    "Deep learning is fundamentally a giant problem in optimisation. We are choosing numerical \"weights\" to minimise a loss function $L$ (which depends on those weights). **This is the learning part.** In other words, \n",
    "$$L(x) = \\sum_{(v, y) \\in \\boldsymbol{E}} \\text{loss}(F(x, v) - y).$$\n",
    "Calculus tells us that the minimizer of $L$ satisfies the following system of equations (there may be many solutions that satisfy this, hence we do not necessarily obtain the minimizer -- we just hope it's something \"good enough\"):\n",
    "\n",
    "> **The partial derivatives of L with respect to the weights $x$ should be zero**: $$\\boxed{\\frac{\\partial L}{\\partial x} = 0 }$$\n",
    "\n",
    "We solve the equation above, iteratively, using a modification of the gradient descent method called **stochastic gradient descent**. \n",
    "\n",
    "*Backpropagation* is a method to compute derivatives quickly, using the chain rule: \n",
    "\n",
    "$$\\frac{dF}{dx} = \\frac{d}{dx}(F_3(F_2(F_1 (x))) = \\frac{dF_3}{dF_2}\\vert_{F_2=F_2(F_1(x))} \\frac{dF_2(F_1(x))}{dF_1}\\vert_{F_1 = F_1(x)} \\frac{dF_1(x)}{dx}\\vert_x.$$\n",
    "\n",
    "A convenient way to visualise how the function $F$ is computed from the weights $x_i$ is to use a **computational graph**. It separates the big computation into small steps, and we can find the derivative of each step (each computation) on the graph.\n",
    "\n",
    "**Backpropagation** is a technique for optimizing parameters in a neural network. There are two types of backpropagation, depending on the relation between the number of inputs and outputs in the neural network: \n",
    "- *Forward-mode*: $F$ has few inputs, but have many outputs \n",
    "- *Backward-mode*: $F$ has many inputs, but have few outputs \n",
    "\n",
    "*Forward-mode* differentiation tracks how one input affects every node. *Reverse-mode* differentiation tracks how every node affects one output. That is, forward-mode differentiation applies the operator $\\dfrac{\\partial (.)}{\\partial x}$ to every node, while reverse mode differentiation applies the operator $\\dfrac{\\partial F}{\\partial (.)}$ to every node. The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18266031",
   "metadata": {
    "id": "18266031"
   },
   "source": [
    "### Computational Graphs\n",
    "\n",
    "<img src=\"handwritten.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "1. For the computational graph above, compute the the $x$ derivative of $F$ using both forward- and backward-mode. Assume that the initial values of $x, y$ are 2 and 3 resp.\n",
    "\n",
    "2. At first it seems unbelievable that reorganising the computation can make such an enormous difference. Let's do an experiment with matrices: consider the product of 3 matrices: $A, B, C$. Which order? $AB$ first or $BC$ first? In other words, should we compute $(AB)C$ or $A(BC)$? \n",
    "\n",
    "Count the number of multiplications in each case and discuss what's the fastest strategy. This can be generalised to a product of $n$ matrices $A_1, A_2, \\dots, A_n$. Both backpropagation, and the *chain matrix multiplication* problem are instances of dynamic programming problems. Colah's blog listed below also points out the connection between dynamic programming and back-propagation. \n",
    "\n",
    "***\n",
    "Source: \n",
    "[1] http://colah.github.io/posts/2015-08-Backprop/\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b085919f",
   "metadata": {
    "id": "b085919f"
   },
   "outputs": [],
   "source": [
    "# let's implement the example from the previous open discussion in pytorch \n",
    "x = Variable(torch.tensor(2.), requires_grad=True)\n",
    "y = Variable(torch.tensor(3.), requires_grad=True)\n",
    "c = x**2 \n",
    "s = x + y \n",
    "F = c*s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a329c3b",
   "metadata": {
    "id": "5a329c3b"
   },
   "outputs": [],
   "source": [
    "F.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24bef211",
   "metadata": {
    "id": "24bef211",
    "outputId": "7cce2968-8561-4eff-f959-4dfa5a55fe9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a84c6e",
   "metadata": {
    "id": "d0a84c6e"
   },
   "outputs": [],
   "source": [
    "F.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a7722",
   "metadata": {
    "id": "208a7722"
   },
   "source": [
    "This is the derivative of $F$ with respect to $x$! Remember that in the backward node, the derivative $\\dfrac{\\partial F}{\\partial (.)}$ is moving backwards. So,\n",
    "```python\n",
    "x.grad\n",
    "``` \n",
    "is the $x$-derivative of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875a8b62",
   "metadata": {
    "id": "875a8b62",
    "outputId": "a9e6ec29-f0d0-481e-a1e9-f1707951954c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(48., dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e2a05",
   "metadata": {
    "id": "e18e2a05"
   },
   "source": [
    "**Exercise**: \n",
    "\n",
    "Let $F = \\log(x) + x^2 y + y^2$. \n",
    "\n",
    "Evaluate $\\dfrac{\\partial{F}}{\\partial{x}}$ and $\\dfrac{\\partial{F}}{\\partial{y}}$ at the point $x = 2$, $y = 3$ by hand (both in forward and backward modes) and by using `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c11eff",
   "metadata": {
    "id": "02c11eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dF/dx: 12.5\n",
      "dF/dy: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Define the variables x and y\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function F\n",
    "F = torch.log(x) + x**2 * y + y**2\n",
    "\n",
    "# Compute gradients\n",
    "F.backward()\n",
    "\n",
    "# Print the gradients\n",
    "x_grad = x.grad\n",
    "y_grad = y.grad\n",
    "\n",
    "print('dF/dx:', x_grad.item())\n",
    "print('dF/dy:', y_grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced715c",
   "metadata": {
    "id": "2ced715c"
   },
   "source": [
    "### The Initial Weights $x_0$ in Gradient Descent \n",
    "\n",
    "The architecture in a neural net decides the form of the learning function $F(x, v)$. The training data goes into $v$. Then we *initialize* the weights $x$ in the matrices $A$ and vectors $b$. From those initial weights $x_0$, the optimisation algorithm (normally a form of gradient descent) computes weights $x_1$, $x_2$ etc aiming to minimizing the total loss iteratively. \n",
    "\n",
    "The million-pounds question is: *What weights $x_0$ should we start with?* Choosing $x_0 = 0$ would be a disaster (why?). Poor initialisation is an important cause of failure in deep learning.  \n",
    "\n",
    "Hanin and Rolnick [1] show that the initial variance $\\sigma^2$ controls the mean of the computed weights. The layer widths controls the variance of the weights. The key point is this: \n",
    "\n",
    "> Many-layered depth can reduce the loss on the training set. But if $\\sigma^2$ is wrong or width is sacrificed, then gradient descent can lose control of the weights. They can explode to infinity or implode to zero. \n",
    "\n",
    "Source:\n",
    "[1] B. Hanin and D. Rolnick, *How to start training: The effect of initialisation and architecture*, arXiv: 1803.01719, 19/06/2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343908e",
   "metadata": {
    "id": "e343908e"
   },
   "source": [
    "### Finding the best weights x: Gradient Descent and Stochastic Gradient Descent \n",
    "\n",
    "#### Gradient Descent toward the mininum \n",
    "\n",
    "> How to minimise a function $f(x_1, x_2, \\dots, x_n)$? \n",
    "\n",
    "Calculus teaches us that all the first derivatives $\\frac{\\partial f}{\\partial x_i}$ are zero at the minimum (when $f$ is smooth). If we have $n=20$ unknowns (a small number in deep learning) then minimising one function $f$ produces 20 equations. *Gradient-descent* uses the derivatives $\\partial f/\\partial x_i$ to find a direction that reduces $f(x)$. \n",
    "\n",
    "> The steepest direction in which $f(x)$ decreases the fastest, is given by $-\\nabla f$: \n",
    "$$\\boxed{\\text{Gradient descent: } x_{k+1} = x_k - s_k \\nabla f(x_k)}\\qquad (\\ast)$$\n",
    "\n",
    "The symbol $\\nabla f$ represents the vector of $n$ partial derivatives of $f$: its *gradient*.  \n",
    "\n",
    "$$\\boxed{\\text{Gradient : } \\nabla f(x_1, x_2, \\dots, x_n) = \\left(\\frac{\\partial F}{\\partial x_1}, \\frac{\\partial F}{\\partial x_2}, \\dots, \\frac{\\partial F}{\\partial x_n} \\right) }$$\n",
    "\n",
    "\n",
    "\n",
    "So the equation $(\\ast)$ above is a vector equation for each step $k = 1, 2, \\dots$ and $s_k$ is the  *stepsize* or the *learning rate*. We hope to move toward the point $x^{\\ast}$ where the graph of $f(x)$ hits the bottom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102df68",
   "metadata": {
    "id": "e102df68"
   },
   "source": [
    "#### Some examples \n",
    "1. For a constant vector $\\mathbf{a} = (a_1, a_2, \\dots, a_n)$, $F(\\mathbf{x}) = \\mathbf{a}^\\intercal \\mathbf{x}$ has gradient \n",
    "$\\nabla F = \\mathbf{a}.$\n",
    "\n",
    "2. For a symmetric matrix $S$, the gradient of $F(\\mathbf{x}) = \\mathbf{x}^{\\intercal} \\mathbf{S} \\mathbf{x}$ is  $\\nabla F = 2 \\mathbf{S} \\mathbf{x}.$\n",
    "\n",
    "3. For a positive definite symmetric matrix $S$, the minimum of a quadratic $F(x)=\\frac{1}{2}x^{\\intercal} S x - a^{\\intercal}x$ is the negative number $F_min = - \\frac{1}{2} a^{\\intercal} S a$  at $x^{\\ast} = S^{-1}a$.\n",
    "\n",
    "4. Let $F(X) = \\det (X)$, the determinant of a square matrix $X$. What do you think the partial derivative $\\frac{\\partial F}{\\partial x_{ij}}$ looks like? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e11fff",
   "metadata": {
    "id": "73e11fff"
   },
   "source": [
    "#### Optimisation with zig-zagging\n",
    "The example $f(x_1, x_2) = \\frac{1}{2}(x^2 + by^2)$ is extremely useful for $0 < b <= 1$. \n",
    "1. Calculate the gradient $\\nabla f$. \n",
    "2. We know that the minimum of $f$ is at $(0,0)$. Suppose instead we try to reach the minimum using the equation $(\\ast)$ above with *exact line search*. That means that at each step we shall choose $s_k$ for which $f$ decreases the most. Show that: \n",
    "$$x_k = b \\bigg (\\frac{b-1}{b+1}\\ \\bigg)^k, y_k=\\bigg(\\frac{1-b}{1+b}\\bigg)^k, f(x_k, y_k) = \\bigg(\\frac{1-b}{1+b}\\bigg)^{2k}f(x_0,y_0),$$\n",
    "where $(x_0, y_0) = (b, 1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf8f30",
   "metadata": {
    "id": "46bf8f30"
   },
   "source": [
    "### Stochastic Gradient Descent and ADAM (optional)\n",
    "\n",
    "Gradient descent is fundamental in training a deep neural network. It is based on a step of the form \n",
    "$$x_{k+1} = x_k - s_k \\nabla L(x_k).$$ That step should lead us downhill toward the point $x^{\\ast}$ where the loss function $L(x)$ is minimised for the test data $v$. But for large networks with many samples in the training set, this algorithm (as it stands) is not sucessful! \n",
    "\n",
    "It's important to recognize two different problems with classical steepest descent: \n",
    "\n",
    "1. Computing $\\nabla L$ (the gradient of the loss) function at every descent step - the derivatives of the total loss $L$ with respect to all the weights $x$ in the network - is too expensive. \n",
    "\n",
    "That total loss adds the individual losses *$l(x, v_i)$ for every sample $v_i$ in the training set* -- potentially millions of separate losses are computed and added in every computation of $L$.\n",
    "\n",
    "2. The number of weights is even larger. So $\\nabla_x L = 0$ for many different choices $x^{\\ast}$ of the weights. **Some of those choices can give poor results on unseen test data.** The learning function $F$ can fail to \"generalise\". But **stochastic gradient descent** (SGD) does find weights $x^{\\ast}$ that generalise. \n",
    "\n",
    "**Stochastic gradient descent uses only a \"minibatch\" of the training data at each step**. $B$ samples will be chosen randomly. Replacing the full batch of all the traiing data by a minibatch changes $L(x) = \\frac{1}{n} \\sum l_i(x)$ to a sum of only $B$ losses. This resolves both difficulties at once. The success of deep learning rests on these two facts: \n",
    "\n",
    "1. Computing $\\nabla l_i$ by backpropagation on B samples is much faster. Often $B$ = 1. \n",
    "2. The stochastic algorithm produces weights $x^{\\ast}$ that also succeed on unseen data. \n",
    "\n",
    "Something remarkable observed in practice is that the *SGD* avoids overfitting. Another fundamental strategy in training a neural network is **early stopping**. We'll provide more details in our practical tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2acc02",
   "metadata": {
    "id": "2d2acc02"
   },
   "source": [
    "#### Stochastic Descent Using One Sample Per Step \n",
    "\n",
    "To simplify, suppose each minibatch contains only one sample $v_k$ (so B = 1). That sample is chosen randomly. The theory of stochastic descent usually assumes that the sample is replaced after use - in principle the sample could be chosen again at step k + 1. In practice, we often omit replacement and work through samples in a random order. \n",
    "\n",
    "Each pass through the training data is **one epoch** of the descent algorithm. Ordinary gradient descent computes one epoch per step (batch mode). Stochastic gradient descent needs many steps (for minibatches). The online advice is to choose $B \\leq 32$. \n",
    "\n",
    "Stochastic descent began with a seminal paper by Robbins and Monro [1] where *they developed a fast method to converge to the desired optimum in probability*: \n",
    "> $\\lim_{k\\to\\infty} Prob(\\vert\\vert x_k - x^{\\ast}\\vert\\vert > \\epsilon) \\to 0.$\n",
    "\n",
    "A word of caution: Stochastic descent is more sensitive to the stepsizes $s_k$ than full gradient descent. A typical feature of stochastic gradient descent is \"semi-convergence\": fast convergence at the start. Improvements we can use for facilitating the convergence of the SGD algorithm at late state iterations are \n",
    "1. adding *momentum* (e.g. Nesterov momentum etc)\n",
    "2. adaptive *learning rates* (e.g. ADAM etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999a96d",
   "metadata": {
    "id": "a999a96d"
   },
   "source": [
    "#### Fast convergence at the start: Least Squares with n = 1 \n",
    "\n",
    "In this case the $i$-ith loss is $l_i = \\frac{1}{2}(a_i x - b_i)^2$ with $a_i > 0$. The gradient of $l_i$ is its derivative $a_i(a_i x - b_i)$. The total loss over all $N$ samples is \n",
    "$$L(x) = \\frac{1}{2N}\\sum (a_i x - b_i)^2,$$\n",
    "which is least squares with $N$ equations and 1 unknown. We can then compute the gradient: \n",
    "\n",
    "$$\\nabla L = \\frac{1}{N} \\sum a_i (a_i x - b_i) = 0.$$ The solution is $x^{\\ast} = \\frac{\\sum a_i b_i}{\\sum a_i^2}.$\n",
    "\n",
    "*Important:* If $B/A$ is the largest ratio $b_i/a_i$, then the true solution $x^{\\ast}$ is below $B/A$. Similarly $x^{\\ast}$ is above the smallest ratio $\\beta/\\alpha$. Therefore if $x_k$ is outside the interval $I$ from $\\beta/\\alpha$ to $B/A$, then the $k$-th gradient descent step will move toward that interval $I$ containing $x^{\\ast}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf17a51",
   "metadata": {
    "id": "9bf17a51",
    "outputId": "387475ca-8061-49ff-b509-b54a6d2e4ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16.81271743774414 [-0.02406471 -2.2378912 ]\n",
      "10 5.905333995819092 [ 0.4373928  -1.79179537]\n",
      "20 1.8725301027297974 [ 0.70951295 -1.54771721]\n",
      "30 2.503507137298584 [ 0.83704448 -1.35265326]\n",
      "40 1.4296228885650635 [ 0.90832788 -1.19260418]\n",
      "50 0.24486276507377625 [ 0.97839952 -1.04354632]\n",
      "60 0.21676556766033173 [ 0.99849898 -0.94356686]\n",
      "70 0.5226307511329651 [ 1.00709164 -0.84442461]\n",
      "80 0.2908508777618408 [ 1.00617743 -0.77556396]\n",
      "90 0.3864205479621887 [ 1.00141335 -0.70546967]\n",
      "100 0.5313148498535156 [ 0.96467346 -0.64023703]\n",
      "110 0.19246816635131836 [ 0.94105804 -0.59466314]\n",
      "120 0.17141762375831604 [ 0.93415409 -0.52809298]\n",
      "130 0.14901241660118103 [ 0.90870672 -0.48698023]\n",
      "140 0.030180707573890686 [ 0.90428048 -0.45444131]\n",
      "150 0.09222254902124405 [ 0.8701337  -0.43882686]\n",
      "160 0.15866123139858246 [ 0.83451885 -0.41398078]\n",
      "170 0.022074252367019653 [ 0.81093192 -0.37333301]\n",
      "180 0.09172672033309937 [ 0.79354519 -0.34541798]\n",
      "190 0.18085487186908722 [ 0.7822423  -0.31247029]\n",
      "200 0.22425810992717743 [ 0.75865269 -0.27602333]\n",
      "210 0.05626818165183067 [ 0.74410087 -0.25820532]\n",
      "220 0.048417698591947556 [ 0.7252652  -0.24076943]\n",
      "230 0.04516948014497757 [ 0.70056856 -0.21608266]\n",
      "240 0.10025953501462936 [ 0.69180369 -0.19028622]\n",
      "250 0.059911929070949554 [ 0.6801582  -0.16792318]\n",
      "260 0.031492866575717926 [ 0.66641098 -0.13867579]\n",
      "270 0.04046299308538437 [ 0.64676034 -0.11925855]\n",
      "280 0.0397738441824913 [ 0.62759298 -0.10937536]\n",
      "290 0.026841962710022926 [ 0.61798084 -0.08871547]\n",
      "300 0.018342453986406326 [ 0.61115611 -0.07165112]\n",
      "310 0.016379693523049355 [ 0.60154378 -0.06252432]\n",
      "320 0.020578959956765175 [ 0.59143198 -0.05385625]\n",
      "330 0.02398284710943699 [ 0.58081222 -0.04257526]\n",
      "340 0.013608992099761963 [ 0.56991696 -0.03832341]\n",
      "350 0.014874211512506008 [ 0.5626781  -0.02576022]\n",
      "360 0.023696554824709892 [ 0.55581754 -0.01485791]\n",
      "370 0.01314384862780571 [ 0.54952115 -0.00151517]\n",
      "380 0.0031166886910796165 [0.54983836 0.00797694]\n",
      "390 0.003215199103578925 [0.54189193 0.01328906]\n",
      "400 0.019160576164722443 [0.54348171 0.02802489]\n",
      "410 0.0037050682585686445 [0.53347307 0.04170541]\n",
      "420 0.005135003011673689 [0.52110946 0.04328089]\n",
      "430 0.016480300575494766 [0.51806247 0.05303727]\n",
      "440 0.001412824378348887 [0.51590317 0.05754857]\n",
      "450 0.004748152103275061 [0.50675219 0.06248731]\n",
      "460 0.0026050021406263113 [0.50246334 0.0676018 ]\n",
      "470 0.0035489187575876713 [0.49581105 0.07264324]\n",
      "480 0.004075109958648682 [0.48979634 0.07780849]\n",
      "490 0.0026335727889090776 [0.48220828 0.08037069]\n",
      "final weights: tensor([[0.4815],\n",
      "        [0.0859]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Code for one-dimensional least squares\n",
    "from torch.autograd import Variable \n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# B is batch size; D_in is input dimension;\n",
    "# N is sample size; D_out is output dimension.\n",
    "B, N, D_in, D_out = 4, 20, 2, 1\n",
    "\n",
    "# We're generating some synthetic data here.\n",
    "# The weights to be learned are (w1, w2) = (1.0, 0.2)\n",
    "# https://stackoverflow.com/questions/17869840/numpy-vector-n-1-dimension-n-dimension-conversion\n",
    "eps = 1.e-2\n",
    "xrange = yrange = np.arange(0.0, 1.0, 0.1)\n",
    "g = np.meshgrid(xrange, yrange, sparse=False, indexing='ij')\n",
    "_x = np.vstack(tup=tuple(map(np.ravel, g))).T\n",
    "_w = np.array((0.4, 0.2)).reshape(1, -1).T\n",
    "_y = _x.dot(_w) + eps * np.random.rand(_x.shape[0], 1)\n",
    "\n",
    "# select a small sample of the data \n",
    "np.random.seed(42)\n",
    "idx = np.random.randint(0, 100, N)\n",
    "x_np = _x[idx]\n",
    "y_np = _y[idx]\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "\n",
    "x = Variable(torch.Tensor(x_np)) \n",
    "y = Variable(torch.Tensor(y_np)) \n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w = torch.randn(D_in, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "epochs = 500\n",
    "weights = np.empty((epochs//10, 2))\n",
    "losses = np.empty(epochs//10)\n",
    "for t in range(epochs):\n",
    "    sample = np.random.randint(0, 20, B)\n",
    "    x_B, y_B = x[sample], y[sample]\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x_B.mm(w)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y_B).pow(2).sum()\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad # this is the gradient of loss with respect to w\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.zero_()\n",
    "        \n",
    "    if t % 10 == 0: \n",
    "        ind = int(t/10)\n",
    "        losses[ind] = loss.item()\n",
    "        weights[ind, :] = w.data.view(1, -1).numpy()[0]\n",
    "        print(t, losses[ind], weights[ind, :])\n",
    "\n",
    "# compare this with the initial weights we had set up in our data \n",
    "print('final weights:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba41d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMP-PCMLAI-m19-advancedML-part3-questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
